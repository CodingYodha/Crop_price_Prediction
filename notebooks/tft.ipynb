{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbf4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temporal Fusion Transformer Forecasting & Validation ---\n",
      "--- (Nashik/Wheat: 2002-2023 Train, 2024 Validate) ---\n",
      "------------------------------\n",
      "Processing Training (2002-2023) Dataset\n",
      "------------------------------\n",
      "Loading Training (2002-2023) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv...\n",
      "Loaded 6246 rows.\n",
      "Constructing 'full_date'...\n",
      "6246 rows after date construction.\n",
      "6246 rows after ensuring price columns numeric.\n",
      "Training (2002-2023) base data loaded. 6246 rows.\n",
      "------------------------------\n",
      "Processing Validation (2024) Dataset\n",
      "------------------------------\n",
      "Loading Validation (2024) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv...\n",
      "Loaded 278 rows.\n",
      "Constructing 'full_date'...\n",
      "278 rows after date construction.\n",
      "278 rows after ensuring price columns numeric.\n",
      "Validation (2024) base data loaded. 278 rows.\n",
      "\n",
      "Selected: Maharashtra/Nashik/Wheat -> Encoded: St=6291, Di=6291, Co=6291\n",
      "\n",
      "Filtering datasets using encoded values...\n",
      "\n",
      "Preparing data for TFT (Target: avg_modal_price)...\n",
      "Creating TimeSeriesDataSet...\n",
      "Error during TFT training or evaluation: Time difference between steps has been idenfied as larger than 1 - set allow_missing_timesteps=True\n",
      "\n",
      "Process finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_22400\\2128141116.py\", line 274, in <module>\n",
      "    training_dataset = TimeSeriesDataSet(\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_forecasting\\data\\timeseries.py\", line 637, in __init__\n",
      "    self.index = self._construct_index(data, predict_mode=self.predict_mode)\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_forecasting\\data\\timeseries.py\", line 1767, in _construct_index\n",
      "    assert self.allow_missing_timesteps, msg\n",
      "AssertionError: Time difference between steps has been idenfied as larger than 1 - set allow_missing_timesteps=True\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from IPython.display import display\n",
    "import traceback\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- PyTorch & Forecasting Imports ---\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer # Example normalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, QuantileLoss # Example metrics for model internal use\n",
    "\n",
    "# Suppress common warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set seeds for reproducibility (optional)\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH_TRAIN = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv\"\n",
    "DATA_PATH_VALIDATION = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv\"\n",
    "\n",
    "TARGET_COLUMNS = ['avg_min_price', 'avg_max_price', 'avg_modal_price'] # We will process one target at a time\n",
    "# Define the primary target for this run (TFT often works best focusing on one main target)\n",
    "# If you want to predict all three, you might need separate models or a multi-target setup (more complex)\n",
    "PRIMARY_TARGET = 'avg_modal_price' # <--- FOCUS ON ONE TARGET FOR SIMPLICITY FIRST\n",
    "\n",
    "DATE_COLUMN = 'full_date' # Name for the combined date column\n",
    "YEAR_COL = 'year'; MONTH_COL = 'month'; DAY_COL = 'date' # Source columns\n",
    "VALIDATION_YEAR = 2024\n",
    "\n",
    "# --- User Selections & Encoding Maps ---\n",
    "SELECTED_STATE_STR = \"Maharashtra\"\n",
    "SELECTED_DISTRICT_STR = \"Nashik\"\n",
    "SELECTED_COMMODITY_STR = \"Wheat\"\n",
    "\n",
    "# Corrected Frequency Encoding Maps provided by user\n",
    "state_name_encoding_map = {\"maharashtra\": 6291}\n",
    "district_name_encoding_map = {\"nashik\": 6291}\n",
    "commodity_name_encoding_map = {\"wheat\": 6291}\n",
    "\n",
    "# --- TFT Configuration ---\n",
    "# How many time steps back model should look\n",
    "# Needs careful tuning based on data frequency and patterns (e.g., ~3-6 months for daily data)\n",
    "ENCODER_LENGTH = 90\n",
    "# How many time steps ahead to predict (relevant if predicting sequences, for single step use 1)\n",
    "# For validation against 2024 data, we predict step-by-step or use the validation length\n",
    "PREDICTION_LENGTH = 1 # Predict one step ahead for validation simplicity for now\n",
    "BATCH_SIZE = 64      # Adjust based on GPU memory\n",
    "NUM_WORKER = 0      # Set > 0 if using multi-processing for dataloaders (can cause issues on Windows)\n",
    "TRAINER_GPUS = 0     # Set to 1 or more if GPU is available (requires CUDA setup)\n",
    "LEARNING_RATE = 0.005 # Example learning rate\n",
    "HIDDEN_SIZE = 32     # Example hidden size for TFT network layers\n",
    "ATTENTION_HEADS = 4  # Example attention heads\n",
    "DROPOUT = 0.1        # Example dropout rate\n",
    "MAX_EPOCHS = 15      # Example max epochs (use EarlyStopping)\n",
    "\n",
    "\n",
    "# --- Helper Functions (Outlier removal - can be skipped if data is clean) ---\n",
    "def remove_outliers_iqr(df, columns_to_check):\n",
    "    # ... (same as before) ...\n",
    "    df_filtered = df.copy(); initial_rows = len(df_filtered)\n",
    "    valid_columns = [col for col in columns_to_check if col in df_filtered.columns and pd.api.types.is_numeric_dtype(df_filtered[col])]\n",
    "    if not valid_columns: return df_filtered\n",
    "    subset_for_iqr = df_filtered[valid_columns]\n",
    "    Q1 = subset_for_iqr.quantile(0.25); Q3 = subset_for_iqr.quantile(0.75); IQR = Q3 - Q1\n",
    "    mask = ~((subset_for_iqr < (Q1 - 1.5 * IQR)) | (subset_for_iqr > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    df_filtered = df_filtered[mask]; rows_removed = initial_rows - len(df_filtered)\n",
    "    if rows_removed > 0: print(f\"Removed {rows_removed} rows via IQR.\")\n",
    "    return df_filtered\n",
    "\n",
    "# --- Data Loading and Preprocessing Function (Adapted for TFT base) ---\n",
    "def load_and_preprocess_base_data(path, date_col_name, year_col, month_col, day_col, target_cols, dataset_name=\"Training\"):\n",
    "    \"\"\"Loads data, constructs date, basic cleaning - NO TFT specific features yet.\"\"\"\n",
    "    print(\"-\" * 30); print(f\"Processing {dataset_name} Dataset\"); print(\"-\" * 30)\n",
    "    try:\n",
    "        print(f\"Loading {dataset_name} data from {path}...\"); df = pd.read_csv(path); print(f\"Loaded {len(df)} rows.\")\n",
    "        # 1. Construct Date\n",
    "        date_components_cols = [year_col, month_col, day_col]\n",
    "        if not all(col in df.columns for col in date_components_cols): print(f\"Error: Date component cols missing: {[c for c in date_components_cols if c not in df.columns]}\"); return None\n",
    "        for col in date_components_cols: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=date_components_cols, inplace=True)\n",
    "        print(f\"Constructing '{date_col_name}'...\");\n",
    "        df[date_col_name] = pd.to_datetime({'year': df[year_col], 'month': df[month_col], 'day': df[day_col]}, errors='coerce')\n",
    "        initial_rows_date = len(df); df.dropna(subset=[date_col_name], inplace=True)\n",
    "        if initial_rows_date > len(df): print(f\"Dropped {initial_rows_date - len(df)} rows due to invalid date components.\")\n",
    "        print(f\"{len(df)} rows after date construction.\")\n",
    "\n",
    "        # 2. Ensure Price columns are numeric (only for the ones we might use)\n",
    "        all_potential_targets = ['avg_min_price', 'avg_max_price', 'avg_modal_price']\n",
    "        for col in all_potential_targets:\n",
    "            if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else: print(f\"Warning: Price column '{col}' not found.\")\n",
    "        df.dropna(subset=all_potential_targets, how='any', inplace=True) # Drop if any price is missing\n",
    "        print(f\"{len(df)} rows after ensuring price columns numeric.\")\n",
    "\n",
    "        # 3. Drop OTHER unused columns\n",
    "        cols_to_drop = ['calculationType', 'district_id', 'change',\n",
    "                        'district_name_enc', 'commodity_name_enc', 'state_name_enc']\n",
    "        existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "        if existing_cols_to_drop: df.drop(columns=existing_cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        # 4. Apply IQR Outlier Removal (Optional)\n",
    "        # df = remove_outliers_iqr(df, all_potential_targets) # Apply if needed\n",
    "\n",
    "        # 5. Check required columns (encoded filters + date + targets)\n",
    "        required_numeric_filter_cols = ['state_name', 'district_name', 'commodity_name']\n",
    "        required_cols = [date_col_name] + all_potential_targets + required_numeric_filter_cols\n",
    "        missing_req_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_req_cols: print(f\"Error: Required columns missing: {missing_req_cols}\"); print(f\"Available: {df.columns.tolist()}\"); return None\n",
    "        for col in required_numeric_filter_cols:\n",
    "             if not pd.api.types.is_numeric_dtype(df[col]): print(f\"Error: Col '{col}' expected numeric but isn't.\"); return None\n",
    "\n",
    "        df.sort_values(date_col_name, inplace=True)\n",
    "        print(f\"{dataset_name} base data loaded. {len(df)} rows.\")\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError: print(f\"Error: {dataset_name} file not found at {path}\"); return None\n",
    "    except Exception as e: print(f\"Error loading/preprocessing {dataset_name}: {e}\"); traceback.print_exc(); return None\n",
    "\n",
    "\n",
    "# --- Evaluation Metrics Function ---\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # ... (same as before) ...\n",
    "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
    "    if len(y_true) == 0 or len(y_pred) == 0: return np.nan, np.nan, np.nan\n",
    "    if len(y_true) != len(y_pred):\n",
    "        min_len = min(len(y_true), len(y_pred)); print(f\"Warn: Mismatch metrics. Truncating to {min_len}.\")\n",
    "        if min_len == 0 : return np.nan, np.nan, np.nan\n",
    "        y_true = y_true[:min_len]; y_pred = y_pred[:min_len]\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred); mae = mean_absolute_error(y_true, y_pred); mse = mean_squared_error(y_true, y_pred)\n",
    "        return r2, mae, mse\n",
    "    except Exception as e: print(f\"Error calculating metrics: {e}\"); return np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "# --- Plotting Function for Validation ---\n",
    "def plot_validation_results(validation_actuals_df, validation_preds_df, target_column, date_col_name, title):\n",
    "    \"\"\"Plots actuals vs predictions for validation period.\"\"\"\n",
    "    fig = go.Figure(); target_label = target_column.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()\n",
    "\n",
    "    # Ensure dataframes have the required columns and are sorted\n",
    "    validation_actuals_df = validation_actuals_df.sort_values(by=date_col_name)\n",
    "    validation_preds_df = validation_preds_df.sort_values(by='time_idx') # preds often keyed by time_idx\n",
    "\n",
    "    # Add Actual Validation Data trace\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=validation_actuals_df[date_col_name],\n",
    "        y=validation_actuals_df[target_column],\n",
    "        mode='lines+markers', name=f'Actual {target_label} ({VALIDATION_YEAR})',\n",
    "        line=dict(color='blue'), marker=dict(size=4)\n",
    "    ))\n",
    "\n",
    "    # Add Predicted Validation Data trace\n",
    "    # Need to align predictions with actual dates if lengths differ slightly\n",
    "    # Assuming predictions correspond to the time indices in validation_actuals_df for simplicity here\n",
    "    # A more robust approach would merge based on time_idx\n",
    "    if len(validation_actuals_df) == len(validation_preds_df):\n",
    "         fig.add_trace(go.Scatter(\n",
    "             x=validation_actuals_df[date_col_name], # Use actual dates for plotting preds\n",
    "             y=validation_preds_df['prediction'].iloc[:, 0], # Assuming single prediction output\n",
    "             mode='lines', name=f'Predicted {target_label} ({VALIDATION_YEAR})',\n",
    "             line=dict(color='red')\n",
    "         ))\n",
    "    else:\n",
    "         print(f\"Warning: Length mismatch in plot data. Actuals: {len(validation_actuals_df)}, Preds: {len(validation_preds_df)}\")\n",
    "         # Attempt plot if possible, might be misaligned\n",
    "         fig.add_trace(go.Scatter(\n",
    "             x=validation_actuals_df[date_col_name].iloc[:len(validation_preds_df)], # Use actual dates for plotting preds\n",
    "             y=validation_preds_df['prediction'].iloc[:len(validation_actuals_df), 0], # Assuming single prediction output\n",
    "             mode='lines', name=f'Predicted {target_label} ({VALIDATION_YEAR} - Potential Mismatch)',\n",
    "             line=dict(color='orange')\n",
    "         ))\n",
    "\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=f'Date ({VALIDATION_YEAR})', yaxis_title=f'Price ({target_label})', hovermode=\"x unified\", legend_title_text='Legend')\n",
    "    return fig\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(\"--- Temporal Fusion Transformer Forecasting & Validation ---\")\n",
    "print(f\"--- (Nashik/Wheat: 2002-2023 Train, {VALIDATION_YEAR} Validate) ---\")\n",
    "\n",
    "# 1. Load Base Data\n",
    "df_train_base = load_and_preprocess_base_data(DATA_PATH_TRAIN, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, \"Training (2002-2023)\")\n",
    "df_val_base = load_and_preprocess_base_data(DATA_PATH_VALIDATION, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, f\"Validation ({VALIDATION_YEAR})\")\n",
    "\n",
    "# Proceed only if both datasets loaded\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "\n",
    "    # 2. Get Encoded Values for Filtering\n",
    "    try:\n",
    "        selected_state_key=SELECTED_STATE_STR.strip().lower(); selected_district_key=SELECTED_DISTRICT_STR.strip().lower(); selected_commodity_key=SELECTED_COMMODITY_STR.strip().lower()\n",
    "        encoded_state = state_name_encoding_map.get(selected_state_key); encoded_district = district_name_encoding_map.get(selected_district_key); encoded_commodity = commodity_name_encoding_map.get(selected_commodity_key)\n",
    "        lookup_failed = False\n",
    "        if encoded_state is None: print(f\"Error: State '{SELECTED_STATE_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_district is None: print(f\"Error: District '{SELECTED_DISTRICT_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_commodity is None: print(f\"Error: Commodity '{SELECTED_COMMODITY_STR}' missing map.\"); lookup_failed=True\n",
    "        if lookup_failed: print(\"Check maps.\"); df_train_base=df_val_base=None\n",
    "        else: print(f\"\\nSelected: {SELECTED_STATE_STR}/{SELECTED_DISTRICT_STR}/{SELECTED_COMMODITY_STR} -> Encoded: St={encoded_state}, Di={encoded_district}, Co={encoded_commodity}\")\n",
    "    except Exception as e: print(f\"Error mapping lookup: {e}\"); df_train_base = df_val_base = None\n",
    "\n",
    "# Proceed only if lookup succeeded\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "\n",
    "    # 3. Filtering Data Based on ENCODED Values\n",
    "    print(f\"\\nFiltering datasets using encoded values...\")\n",
    "    filter_cols_num = ['state_name', 'district_name', 'commodity_name']\n",
    "\n",
    "    # Filter Training Data\n",
    "    if not all(col in df_train_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Training.\"); filtered_df_train = pd.DataFrame()\n",
    "    else:\n",
    "        filtered_df_train = df_train_base[(df_train_base['state_name'] == encoded_state) & (df_train_base['district_name'] == encoded_district) & (df_train_base['commodity_name'] == encoded_commodity)].copy()\n",
    "        filtered_df_train.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "\n",
    "    # Filter Validation Data\n",
    "    if not all(col in df_val_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Validation.\"); filtered_df_val = pd.DataFrame()\n",
    "    else:\n",
    "        filtered_df_val = df_val_base[(df_val_base['state_name'] == encoded_state) & (df_val_base['district_name'] == encoded_district) & (df_val_base['commodity_name'] == encoded_commodity)].copy()\n",
    "        filtered_df_val.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "\n",
    "    if filtered_df_train.empty: print(\"\\nWarning: No training data after filtering.\")\n",
    "    if filtered_df_val.empty: print(\"\\nWarning: No validation data after filtering.\")\n",
    "\n",
    "    # 4. Prepare Data for TFT (Focusing on PRIMARY_TARGET)\n",
    "    if not filtered_df_train.empty and not filtered_df_val.empty:\n",
    "        print(f\"\\nPreparing data for TFT (Target: {PRIMARY_TARGET})...\")\n",
    "\n",
    "        # Combine temporarily for global time_idx calculation\n",
    "        filtered_df_train[\"dataset_type\"] = \"train\"\n",
    "        filtered_df_val[\"dataset_type\"] = \"val\"\n",
    "        # Ensure validation data follows training data directly for time_idx continuity\n",
    "        data_comb = pd.concat([filtered_df_train, filtered_df_val], ignore_index=True)\n",
    "        data_comb = data_comb.sort_values(DATE_COLUMN)\n",
    "\n",
    "        # Create time index\n",
    "        data_comb['time_idx'] = (data_comb[DATE_COLUMN] - data_comb[DATE_COLUMN].min()).dt.days\n",
    "\n",
    "        # Create group ID (only one series after filtering for Nashik/Wheat)\n",
    "        data_comb['group_id'] = f\"{SELECTED_DISTRICT_STR}_{SELECTED_COMMODITY_STR}_{PRIMARY_TARGET}\"\n",
    "\n",
    "        # Add time features (Known Future Inputs) - ensure they are strings for categorical embedding\n",
    "        data_comb['month'] = data_comb[DATE_COLUMN].dt.month.astype(str)\n",
    "        data_comb['day_of_week'] = data_comb[DATE_COLUMN].dt.dayofweek.astype(str)\n",
    "        data_comb['day_of_year'] = data_comb[DATE_COLUMN].dt.dayofyear.astype(str)\n",
    "        # Add year as real (continuous) known input\n",
    "        data_comb['year_real'] = data_comb[DATE_COLUMN].dt.year\n",
    "\n",
    "        # Convert target to float32 (recommended)\n",
    "        data_comb[PRIMARY_TARGET] = data_comb[PRIMARY_TARGET].astype(np.float32)\n",
    "\n",
    "        # Add lags or other features if desired (Time Varying Unknown)\n",
    "        # Example: data_comb[f'{PRIMARY_TARGET}_lag1'] = data_comb.groupby('group_id')[PRIMARY_TARGET].shift(1)\n",
    "        # data_comb = data_comb.dropna(subset=[f'{PRIMARY_TARGET}_lag1']) # Drop rows with NaN lags\n",
    "\n",
    "        # Separate back into train and validation sets\n",
    "        train_data_tft = data_comb[data_comb[\"dataset_type\"] == \"train\"].copy()\n",
    "        val_data_tft = data_comb[data_comb[\"dataset_type\"] == \"val\"].copy()\n",
    "\n",
    "        # --- Create TimeSeriesDataSet ---\n",
    "        print(\"Creating TimeSeriesDataSet...\")\n",
    "        try:\n",
    "            # Define Target Normalizer (fitted only on training data)\n",
    "            target_normalizer = GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\") # or \"standard\" or None\n",
    "\n",
    "            training_cutoff = train_data_tft[\"time_idx\"].max() # Last time index in training data\n",
    "\n",
    "            training_dataset = TimeSeriesDataSet(\n",
    "                train_data_tft,\n",
    "                time_idx=\"time_idx\",\n",
    "                target=PRIMARY_TARGET,\n",
    "                group_ids=[\"group_id\"],\n",
    "                max_encoder_length=ENCODER_LENGTH,\n",
    "                max_prediction_length=PREDICTION_LENGTH,\n",
    "                # Static features (only group_id here as others are constant)\n",
    "                static_categoricals=[\"group_id\"],\n",
    "                # Time-varying known features\n",
    "                time_varying_known_categoricals=[\"month\", \"day_of_week\", \"day_of_year\"],\n",
    "                time_varying_known_reals=[\"year_real\", \"time_idx\"], # time_idx needed as real feature\n",
    "                # Time-varying unknown features (add lags here if created)\n",
    "                time_varying_unknown_reals=[PRIMARY_TARGET], # Target needs to be here if no lags/other unknowns\n",
    "                target_normalizer=target_normalizer,\n",
    "                add_relative_time_idx=True, # Recommended\n",
    "                add_target_scales=True,     # Recommended\n",
    "                add_encoder_length=True,\n",
    "                allow_missing_timesteps=True    # Recommended\n",
    "            )\n",
    "\n",
    "            # Create validation dataset using data from training dataset\n",
    "            # Crucial: allow_missings=True needed because validation starts right after train\n",
    "            validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "                training_dataset,\n",
    "                data_comb[data_comb[\"time_idx\"] > training_cutoff - ENCODER_LENGTH], # Need encoder length overlap\n",
    "                predict=False, # Change to True if only predicting, False if evaluating loss\n",
    "                stop_randomization=True,\n",
    "                # allow_missing_timesteps=True # Use if gaps exist, implies predict=True needed? Check docs\n",
    "            )\n",
    "\n",
    "            # Create dataloaders\n",
    "            train_dataloader = training_dataset.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKER)\n",
    "            val_dataloader = validation_dataset.to_dataloader(train=False, batch_size=BATCH_SIZE * 2, num_workers=NUM_WORKER) # Larger batch size for validation often ok\n",
    "\n",
    "            print(\"TimeSeriesDataSet and Dataloaders created.\")\n",
    "\n",
    "            # --- 5. Define Model & Trainer ---\n",
    "            print(\"\\nDefining TFT model and Trainer...\")\n",
    "            # Define callbacks\n",
    "            early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=True, mode=\"min\")\n",
    "            lr_monitor = LearningRateMonitor()\n",
    "            # logger = TensorBoardLogger(\"tb_logs\", name=\"tft_wheat_price\") # Optional: for TensorBoard logging\n",
    "\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=MAX_EPOCHS,\n",
    "                gpus=TRAINER_GPUS,\n",
    "                gradient_clip_val=0.1, # Helps prevent exploding gradients\n",
    "                limit_train_batches=30,  # Limit batches per epoch for faster iteration/debugging (remove for full run)\n",
    "                limit_val_batches=10,   # Limit validation batches (remove for full run)\n",
    "                # fast_dev_run=True, # Uncomment for quick test run (1 batch train/val)\n",
    "                callbacks=[lr_monitor, early_stop_callback],\n",
    "                # logger=logger,\n",
    "                # progress_bar_refresh_rate=30 # Deprecated, use enable_progress_bar=True\n",
    "                enable_progress_bar=True\n",
    "            )\n",
    "\n",
    "            # Define TFT model from dataset parameters\n",
    "            tft = TemporalFusionTransformer.from_dataset(\n",
    "                training_dataset,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                hidden_size=HIDDEN_SIZE,\n",
    "                attention_head_size=ATTENTION_HEADS,\n",
    "                dropout=DROPOUT,\n",
    "                hidden_continuous_size=HIDDEN_SIZE // 2, # Example configuration\n",
    "                output_size=7,  # Number of quantiles to predict (usually 7 for P10, P50, P90 etc.)\n",
    "                loss=QuantileLoss(), # Use QuantileLoss for probabilistic forecasts\n",
    "                # reduce_on_plateau_patience=4 # Adjust LR scheduler patience\n",
    "            )\n",
    "            print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "            # --- 6. Train Model ---\n",
    "            print(\"\\nStarting model training...\")\n",
    "            trainer.fit(\n",
    "                tft,\n",
    "                train_dataloaders=train_dataloader,\n",
    "                val_dataloaders=val_dataloader,\n",
    "            )\n",
    "            print(\"Training finished.\")\n",
    "\n",
    "            # --- 7. Evaluate on Validation Set ---\n",
    "            print(f\"\\n--- Evaluating FINAL TFT Model Performance on {VALIDATION_YEAR} Validation Data ---\")\n",
    "            # Load best model checkpoint\n",
    "            best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "            print(f\"Loading best model from: {best_model_path}\")\n",
    "            best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "            # Predict on validation data\n",
    "            # Predict using the dataloader to automatically handle encoder/decoder lengths\n",
    "            print(\"Predicting on validation data...\")\n",
    "            predictions_raw = best_tft.predict(val_dataloader, return_index=True, return_decoder_lengths=True)\n",
    "            # predictions_raw object contains 'prediction', 'index', 'decoder_lengths'\n",
    "\n",
    "            # Extract actuals corresponding to the predictions\n",
    "            actuals_raw = torch.cat([y[0] for _, y in iter(val_dataloader)])\n",
    "            # Align actuals and predictions using the index returned by predict\n",
    "            # This part can be tricky, ensure alignment based on time_idx and group_id\n",
    "            # For a single series, alignment might be simpler if lengths match\n",
    "            actuals_flat = actuals_raw.view(-1) # Flatten the actuals tensor\n",
    "            # Predictions are often quantile forecasts, take the median (index 3 for 7 quantiles)\n",
    "            predictions_median = predictions_raw.prediction[:, :, 3].view(-1) # Flatten the median prediction tensor\n",
    "\n",
    "            # --- Ensure lengths match for metric calculation ---\n",
    "            min_len_eval = min(len(actuals_flat), len(predictions_median))\n",
    "            if len(actuals_flat) != len(predictions_median):\n",
    "                 print(f\"Warning: Mismatch evaluation lengths. Actuals: {len(actuals_flat)}, Preds: {len(predictions_median)}. Truncating.\")\n",
    "            actuals_aligned = actuals_flat[:min_len_eval].cpu().numpy() # Move to CPU and convert to numpy\n",
    "            preds_aligned = predictions_median[:min_len_eval].cpu().numpy()\n",
    "\n",
    "            # Inverse transform (if normalizer was used)\n",
    "            # Get the normalizer fitted on the training data\n",
    "            target_scaler = training_dataset.target_normalizer\n",
    "\n",
    "            # Check if inverse_transform expects shape (n_samples, 1) or just (n_samples,)\n",
    "            # Reshape if necessary based on the specific normalizer\n",
    "            # Example for GroupNormalizer - may need adjustments\n",
    "            # Create dummy group info matching the structure expected by the normalizer\n",
    "            inverse_transform_df_actuals = pd.DataFrame({\n",
    "                PRIMARY_TARGET: actuals_aligned,\n",
    "                'group_id': [training_dataset.decoded_index.group_ids.iloc[0]] * len(actuals_aligned) # Use the group id\n",
    "            })\n",
    "            inverse_transform_df_preds = pd.DataFrame({\n",
    "                PRIMARY_TARGET: preds_aligned,\n",
    "                'group_id': [training_dataset.decoded_index.group_ids.iloc[0]] * len(preds_aligned)\n",
    "            })\n",
    "\n",
    "            actuals_inv = target_scaler.inverse_transform(inverse_transform_df_actuals)[PRIMARY_TARGET].values\n",
    "            preds_inv = target_scaler.inverse_transform(inverse_transform_df_preds)[PRIMARY_TARGET].values\n",
    "\n",
    "            # Calculate metrics on inverse-transformed data\n",
    "            if len(actuals_inv) > 0 and len(preds_inv) > 0 :\n",
    "                r2_val, mae_val, mse_val = calculate_metrics(actuals_inv, preds_inv)\n",
    "                print(f\"FINAL Validation R-squared (R2): {r2_val:.4f}\")\n",
    "                print(f\"FINAL Validation Mean Absolute Error (MAE): {mae_val:.2f}\")\n",
    "                print(f\"FINAL Validation Mean Squared Error (MSE): {mse_val:.2f}\")\n",
    "\n",
    "                # --- 8. Plot Validation Results ---\n",
    "                print(f\"\\n--- Plotting FINAL Validation Results for {PRIMARY_TARGET} (Actual vs. Predicted {VALIDATION_YEAR}) ---\")\n",
    "                # Create dataframes for plotting (using inverse transformed values)\n",
    "                # Need to associate predictions back with dates\n",
    "                # Use the index returned by predict() which contains time_idx and group_id\n",
    "                plot_preds_df = predictions_raw.index.copy()\n",
    "                plot_preds_df['prediction'] = pd.DataFrame(preds_inv.reshape(-1,1)) # Reshape to 2D for consistency if needed\n",
    "                # Map time_idx back to date using the original validation data\n",
    "                time_idx_to_date = val_data_tft[[DATE_COLUMN, 'time_idx']].set_index('time_idx')\n",
    "                plot_preds_df = plot_preds_df.join(time_idx_to_date, on='time_idx')\n",
    "\n",
    "                # Actuals for plotting (already inverse transformed)\n",
    "                plot_actuals_df = val_data_tft.iloc[:len(actuals_inv)].copy() # Ensure length matches evaluation\n",
    "                plot_actuals_df[PRIMARY_TARGET] = actuals_inv\n",
    "\n",
    "                plot_title_val = f'TFT Validation (Nashik/Wheat): {PRIMARY_TARGET.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()} Price (Actual vs. Predicted {VALIDATION_YEAR})'\n",
    "                fig_val = plot_validation_results(plot_actuals_df, plot_preds_df, PRIMARY_TARGET, DATE_COLUMN, plot_title_val)\n",
    "                fig_val.show()\n",
    "            else:\n",
    "                 print(\"Skipping metrics and plotting due to empty aligned actuals/predictions.\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during TFT training or evaluation: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    else:\n",
    "         print(\"\\nCannot proceed: lack of data after filtering.\")\n",
    "else:\n",
    "    print(\"\\nFailed during data loading, preprocessing, or mapping lookup.\")\n",
    "\n",
    "print(\"\\nProcess finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe9107",
   "metadata": {},
   "source": [
    "## 2nd try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c8dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Temporal Fusion Transformer Forecasting & Validation ---\n",
      "--- (Nashik/Wheat: 2002-2023 Train, 2024 Validate) ---\n",
      "------------------------------\n",
      "Processing Training (2002-2023) Dataset\n",
      "------------------------------\n",
      "Loading Training (2002-2023) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv...\n",
      "Loaded 6246 rows.\n",
      "Constructing 'full_date'...\n",
      "6246 rows after date construction.\n",
      "6246 rows after ensuring price columns numeric.\n",
      "Training (2002-2023) base data loaded. 6246 rows.\n",
      "------------------------------\n",
      "Processing Validation (2024) Dataset\n",
      "------------------------------\n",
      "Loading Validation (2024) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv...\n",
      "Loaded 278 rows.\n",
      "Constructing 'full_date'...\n",
      "278 rows after date construction.\n",
      "278 rows after ensuring price columns numeric.\n",
      "Validation (2024) base data loaded. 278 rows.\n",
      "\n",
      "Selected: Maharashtra/Nashik/Wheat -> Encoded: St=6291, Di=6291, Co=6291\n",
      "\n",
      "Filtering datasets using encoded values...\n",
      "\n",
      "Preparing data for TFT (Target: avg_modal_price)...\n",
      "Creating TimeSeriesDataSet...\n",
      "TimeSeriesDataSet and Dataloaders created.\n",
      "\n",
      "Defining TFT model and Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFT Network parameters: 75.9k\n",
      "\n",
      "Starting model training...\n",
      "Error during TFT training or evaluation: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`\n",
      "\n",
      "Process finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_22884\\92919337.py\", line 327, in <module>\n",
      "    trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
      "  File \"c:\\Users\\Shiva\\.conda\\envs\\tft_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 554, in fit\n",
      "    model = _maybe_unwrap_optimized(model)\n",
      "  File \"c:\\Users\\Shiva\\.conda\\envs\\tft_env\\lib\\site-packages\\pytorch_lightning\\utilities\\compile.py\", line 111, in _maybe_unwrap_optimized\n",
      "    raise TypeError(\n",
      "TypeError: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-forecasting pytorch-lightning -U # Uncomment if needed\n",
    "\n",
    "# --- Standard Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from IPython.display import display\n",
    "import traceback\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- PyTorch & Forecasting Imports ---\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger # Optional\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, QuantileLoss\n",
    "# Correct import for TemporalFusionTransformer\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "\n",
    "# Suppress common warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set seeds for reproducibility (optional)\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Data Paths (Ensure these point to your pre-encoded files)\n",
    "DATA_PATH_TRAIN = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv\"\n",
    "DATA_PATH_VALIDATION = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv\"\n",
    "\n",
    "# Target Columns & Date Construction Columns\n",
    "TARGET_COLUMNS = ['avg_min_price', 'avg_max_price', 'avg_modal_price']\n",
    "PRIMARY_TARGET = 'avg_modal_price' # Focus on one target for TFT simplicity\n",
    "DATE_COLUMN = 'full_date' # Name for the constructed date column\n",
    "YEAR_COL = 'year'; MONTH_COL = 'month'; DAY_COL = 'date' # Source columns for date\n",
    "VALIDATION_YEAR = 2024\n",
    "\n",
    "# Filter Selections\n",
    "SELECTED_STATE_STR = \"Maharashtra\"\n",
    "SELECTED_DISTRICT_STR = \"Nashik\"\n",
    "SELECTED_COMMODITY_STR = \"Wheat\"\n",
    "\n",
    "# Frequency Encoding Maps (CRITICAL: CONFIRM THESE ARE CORRECT FOR YOUR FILES)\n",
    "state_name_encoding_map = {\"maharashtra\": 6291}\n",
    "district_name_encoding_map = {\"nashik\": 6291}\n",
    "commodity_name_encoding_map = {\"wheat\": 6291}\n",
    "\n",
    "# TFT Configuration\n",
    "ENCODER_LENGTH = 90      # Input sequence length (e.g., 90 days) - NEEDS TUNING\n",
    "PREDICTION_LENGTH = 1    # Predict 1 step ahead for this validation setup\n",
    "BATCH_SIZE = 64          # Adjust based on memory\n",
    "NUM_WORKER = 0           # Use 0 for Windows generally\n",
    "TRAINER_GPUS = 0         # Set to 1+ if GPU available\n",
    "LEARNING_RATE = 0.005    # Example, needs tuning\n",
    "HIDDEN_SIZE = 32         # Example, needs tuning\n",
    "ATTENTION_HEADS = 4      # Example, needs tuning\n",
    "DROPOUT = 0.1            # Example, needs tuning\n",
    "MAX_EPOCHS = 15          # Example, use EarlyStopping\n",
    "\n",
    "# --- Outlier Removal Function ---\n",
    "def remove_outliers_iqr(df, columns_to_check):\n",
    "    \"\"\"Removes outliers from specified numerical columns using the IQR method.\"\"\"\n",
    "    df_filtered = df.copy(); initial_rows = len(df_filtered)\n",
    "    valid_columns = [col for col in columns_to_check if col in df_filtered.columns and pd.api.types.is_numeric_dtype(df_filtered[col])]\n",
    "    if not valid_columns: return df_filtered\n",
    "    subset_for_iqr = df_filtered[valid_columns]\n",
    "    Q1 = subset_for_iqr.quantile(0.25); Q3 = subset_for_iqr.quantile(0.75); IQR = Q3 - Q1\n",
    "    mask = ~((subset_for_iqr < (Q1 - 1.5 * IQR)) | (subset_for_iqr > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    df_filtered = df_filtered[mask]; rows_removed = initial_rows - len(df_filtered)\n",
    "    if rows_removed > 0: print(f\"Removed {rows_removed} rows via IQR.\")\n",
    "    return df_filtered\n",
    "\n",
    "# --- Data Loading and Preprocessing Function ---\n",
    "def load_and_preprocess_base_data(path, date_col_name, year_col, month_col, day_col, target_cols, dataset_name=\"Training\"):\n",
    "    \"\"\"Loads data, constructs date, basic cleaning.\"\"\"\n",
    "    print(\"-\" * 30); print(f\"Processing {dataset_name} Dataset\"); print(\"-\" * 30)\n",
    "    try:\n",
    "        print(f\"Loading {dataset_name} data from {path}...\"); df = pd.read_csv(path); print(f\"Loaded {len(df)} rows.\")\n",
    "        # 1. Construct Date\n",
    "        date_components_cols = [year_col, month_col, day_col]\n",
    "        if not all(col in df.columns for col in date_components_cols): print(f\"Error: Date component cols missing: {[c for c in date_components_cols if c not in df.columns]}\"); return None\n",
    "        for col in date_components_cols: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=date_components_cols, inplace=True)\n",
    "        print(f\"Constructing '{date_col_name}'...\");\n",
    "        df[date_col_name] = pd.to_datetime({'year': df[year_col], 'month': df[month_col], 'day': df[day_col]}, errors='coerce')\n",
    "        initial_rows_date = len(df); df.dropna(subset=[date_col_name], inplace=True)\n",
    "        if initial_rows_date > len(df): print(f\"Dropped {initial_rows_date - len(df)} rows due to invalid date components.\")\n",
    "        print(f\"{len(df)} rows after date construction.\")\n",
    "\n",
    "        # 2. Ensure Price columns are numeric\n",
    "        all_potential_targets = ['avg_min_price', 'avg_max_price', 'avg_modal_price']\n",
    "        for col in all_potential_targets:\n",
    "            if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else: print(f\"Warning: Price column '{col}' not found.\")\n",
    "        df.dropna(subset=all_potential_targets, how='any', inplace=True) # Drop if any price is missing\n",
    "        print(f\"{len(df)} rows after ensuring price columns numeric.\")\n",
    "\n",
    "        # 3. Drop OTHER unused columns\n",
    "        cols_to_drop = ['calculationType', 'district_id', 'change','district_name_enc', 'commodity_name_enc', 'state_name_enc']\n",
    "        existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "        if existing_cols_to_drop: df.drop(columns=existing_cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "        # 4. Apply IQR Outlier Removal (Optional)\n",
    "        # df = remove_outliers_iqr(df, all_potential_targets)\n",
    "\n",
    "        # 5. Check required columns (encoded filters + date + targets)\n",
    "        required_numeric_filter_cols = ['state_name', 'district_name', 'commodity_name']\n",
    "        required_cols = [date_col_name] + all_potential_targets + required_numeric_filter_cols\n",
    "        missing_req_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_req_cols: print(f\"Error: Required columns missing: {missing_req_cols}\"); print(f\"Available: {df.columns.tolist()}\"); return None\n",
    "        for col in required_numeric_filter_cols:\n",
    "             if not pd.api.types.is_numeric_dtype(df[col]): print(f\"Error: Col '{col}' expected numeric but isn't.\"); return None\n",
    "\n",
    "        df.sort_values(date_col_name, inplace=True)\n",
    "        print(f\"{dataset_name} base data loaded. {len(df)} rows.\")\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError: print(f\"Error: {dataset_name} file not found at {path}\"); return None\n",
    "    except Exception as e: print(f\"Error loading/preprocessing {dataset_name}: {e}\"); traceback.print_exc(); return None\n",
    "\n",
    "\n",
    "# --- Evaluation Metrics Function ---\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculates R2, MAE, MSE after handling potential NaNs and length mismatches.\"\"\"\n",
    "    y_true = np.array(y_true).flatten(); y_pred = np.array(y_pred).flatten()\n",
    "    # Remove rows with NaN in either array after alignment\n",
    "    valid_mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true = y_true[valid_mask]; y_pred = y_pred[valid_mask]\n",
    "    if len(y_true) == 0: print(\"Warning: No valid (non-NaN) points for metric calculation.\"); return np.nan, np.nan, np.nan\n",
    "    if len(y_true) != len(y_pred): # Should ideally not happen if preprocessing is correct\n",
    "        min_len = min(len(y_true), len(y_pred)); print(f\"Warn: Mismatch metrics post-NaN. Truncating to {min_len}.\")\n",
    "        if min_len == 0 : return np.nan, np.nan, np.nan\n",
    "        y_true = y_true[:min_len]; y_pred = y_pred[:min_len]\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred); mae = mean_absolute_error(y_true, y_pred); mse = mean_squared_error(y_true, y_pred)\n",
    "        return r2, mae, mse\n",
    "    except Exception as e: print(f\"Error calculating metrics: {e}\"); return np.nan, np.nan, np.nan\n",
    "\n",
    "# --- Plotting Function for Validation ---\n",
    "def plot_validation_results(validation_actuals_df, validation_preds_df, target_column, date_col_name, title):\n",
    "    \"\"\"Plots actuals vs predictions for validation period.\"\"\"\n",
    "    fig = go.Figure(); target_label = target_column.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()\n",
    "    # Ensure dataframes have the required columns and are sorted\n",
    "    plot_actuals_df = validation_actuals_df.sort_values(by=date_col_name).copy()\n",
    "    plot_preds_df = validation_preds_df.sort_values(by='time_idx').copy() # preds often keyed by time_idx\n",
    "\n",
    "    # Add Actual Validation Data trace\n",
    "    fig.add_trace(go.Scatter(x=plot_actuals_df[date_col_name], y=plot_actuals_df[target_column], mode='lines+markers', name=f'Actual {target_label} ({VALIDATION_YEAR})', line=dict(color='blue'), marker=dict(size=4)))\n",
    "\n",
    "    # Add Predicted Validation Data trace - Use dates from prediction index if possible\n",
    "    # Merge predicted values back onto a date axis for plotting\n",
    "    if date_col_name in plot_preds_df.columns:\n",
    "        plot_preds_df_dated = plot_preds_df\n",
    "    else: # If date column is missing in preds, merge it back based on time_idx\n",
    "         time_idx_to_date = plot_actuals_df[[date_col_name, 'time_idx']].drop_duplicates().set_index('time_idx')\n",
    "         plot_preds_df_dated = plot_preds_df.join(time_idx_to_date, on='time_idx')\n",
    "\n",
    "    # Ensure prediction column exists and handle potential multi-output (take median)\n",
    "    pred_col_name = 'prediction'\n",
    "    if pred_col_name in plot_preds_df_dated.columns:\n",
    "         if plot_preds_df_dated[pred_col_name].ndim > 1 and plot_preds_df_dated[pred_col_name].shape[1] > 1:\n",
    "              # Assuming quantile forecasts, take median (index 3 for 7 quantiles P10,P25,P50,P75,P90)\n",
    "              pred_values = plot_preds_df_dated[pred_col_name].iloc[:, 3]\n",
    "              print(\"Plotting median (quantile 0.5) prediction.\")\n",
    "         else:\n",
    "              pred_values = plot_preds_df_dated[pred_col_name].iloc[:, 0] # Single prediction output\n",
    "    else:\n",
    "         print(f\"Warning: Prediction column '{pred_col_name}' not found in prediction results for plotting.\")\n",
    "         pred_values = pd.Series(dtype=float) # Empty series\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=plot_preds_df_dated[date_col_name],\n",
    "        y=pred_values,\n",
    "        mode='lines', name=f'Predicted {target_label} ({VALIDATION_YEAR})',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=f'Date ({VALIDATION_YEAR})', yaxis_title=f'Price ({target_label})', hovermode=\"x unified\", legend_title_text='Legend')\n",
    "    return fig\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(\"--- Temporal Fusion Transformer Forecasting & Validation ---\")\n",
    "print(f\"--- (Nashik/Wheat: 2002-2023 Train, {VALIDATION_YEAR} Validate) ---\")\n",
    "\n",
    "# 1. Load Base Data\n",
    "df_train_base = load_and_preprocess_base_data(DATA_PATH_TRAIN, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, \"Training (2002-2023)\")\n",
    "df_val_base = load_and_preprocess_base_data(DATA_PATH_VALIDATION, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, f\"Validation ({VALIDATION_YEAR})\")\n",
    "\n",
    "# Init flags/variables for later checks\n",
    "train_dataloader = None\n",
    "val_dataloader = None\n",
    "training_dataset = None # To access normalizer later\n",
    "\n",
    "# Proceed only if both datasets loaded successfully\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "\n",
    "    # 2. Get Encoded Values for Filtering\n",
    "    try:\n",
    "        selected_state_key=SELECTED_STATE_STR.strip().lower(); selected_district_key=SELECTED_DISTRICT_STR.strip().lower(); selected_commodity_key=SELECTED_COMMODITY_STR.strip().lower()\n",
    "        encoded_state = state_name_encoding_map.get(selected_state_key); encoded_district = district_name_encoding_map.get(selected_district_key); encoded_commodity = commodity_name_encoding_map.get(selected_commodity_key)\n",
    "        lookup_failed = False\n",
    "        if encoded_state is None: print(f\"Error: State '{SELECTED_STATE_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_district is None: print(f\"Error: District '{SELECTED_DISTRICT_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_commodity is None: print(f\"Error: Commodity '{SELECTED_COMMODITY_STR}' missing map.\"); lookup_failed=True\n",
    "        if lookup_failed: print(\"Check maps.\"); df_train_base=df_val_base=None\n",
    "        else: print(f\"\\nSelected: {SELECTED_STATE_STR}/{SELECTED_DISTRICT_STR}/{SELECTED_COMMODITY_STR} -> Encoded: St={encoded_state}, Di={encoded_district}, Co={encoded_commodity}\")\n",
    "    except Exception as e: print(f\"Error mapping lookup: {e}\"); df_train_base = df_val_base = None\n",
    "\n",
    "# Proceed only if lookup succeeded\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "\n",
    "    # 3. Filtering Data Based on ENCODED Values\n",
    "    print(f\"\\nFiltering datasets using encoded values...\")\n",
    "    filter_cols_num = ['state_name', 'district_name', 'commodity_name']\n",
    "    if not all(col in df_train_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Training.\"); filtered_df_train = pd.DataFrame()\n",
    "    else: filtered_df_train = df_train_base[(df_train_base['state_name'] == encoded_state) & (df_train_base['district_name'] == encoded_district) & (df_train_base['commodity_name'] == encoded_commodity)].copy(); filtered_df_train.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "    if not all(col in df_val_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Validation.\"); filtered_df_val = pd.DataFrame()\n",
    "    else: filtered_df_val = df_val_base[(df_val_base['state_name'] == encoded_state) & (df_val_base['district_name'] == encoded_district) & (df_val_base['commodity_name'] == encoded_commodity)].copy(); filtered_df_val.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "\n",
    "    if filtered_df_train.empty: print(\"\\nWarning: No training data after filtering.\")\n",
    "    if filtered_df_val.empty: print(\"\\nWarning: No validation data after filtering.\")\n",
    "\n",
    "    # 4. Prepare Data for TFT (Focusing on PRIMARY_TARGET)\n",
    "    if not filtered_df_train.empty and not filtered_df_val.empty:\n",
    "        print(f\"\\nPreparing data for TFT (Target: {PRIMARY_TARGET})...\")\n",
    "        try:\n",
    "            # Combine temporarily for global time_idx\n",
    "            filtered_df_train[\"dataset_type\"] = \"train\"; filtered_df_val[\"dataset_type\"] = \"val\"\n",
    "            data_comb = pd.concat([filtered_df_train, filtered_df_val], ignore_index=True).sort_values(DATE_COLUMN)\n",
    "            data_comb['time_idx'] = (data_comb[DATE_COLUMN] - data_comb[DATE_COLUMN].min()).dt.days\n",
    "            data_comb['group_id'] = f\"{SELECTED_DISTRICT_STR}_{SELECTED_COMMODITY_STR}_{PRIMARY_TARGET}\" # Unique ID for this series/target\n",
    "            # Add time features (Known Future Inputs)\n",
    "            data_comb['month'] = data_comb[DATE_COLUMN].dt.month.astype(str)\n",
    "            data_comb['day_of_week'] = data_comb[DATE_COLUMN].dt.dayofweek.astype(str)\n",
    "            data_comb['day_of_month'] = data_comb[DATE_COLUMN].dt.day.astype(str) # Added day of month\n",
    "            # data_comb['day_of_year'] = data_comb[DATE_COLUMN].dt.dayofyear.astype(str) # Can add if useful\n",
    "            data_comb['week_of_year'] = data_comb[DATE_COLUMN].dt.isocalendar().week.astype(str) # Added week\n",
    "            # data_comb['year_cat'] = data_comb[DATE_COLUMN].dt.year.astype(str) # Year also as categorical?\n",
    "            data_comb['year_real'] = data_comb[DATE_COLUMN].dt.year # Year as real\n",
    "            data_comb[PRIMARY_TARGET] = data_comb[PRIMARY_TARGET].astype(np.float32) # Ensure target is float32\n",
    "\n",
    "            # --- Feature list definitions ---\n",
    "            time_varying_known_categoricals = [\"month\", \"day_of_week\", \"day_of_month\", \"week_of_year\"]\n",
    "            time_varying_known_reals = [\"year_real\", \"time_idx\"]\n",
    "            # Add lags or other external features here if needed:\n",
    "            time_varying_unknown_reals = [PRIMARY_TARGET] # If no other unknown features, target is needed here\n",
    "            static_categoricals = [\"group_id\"]\n",
    "\n",
    "            # Separate back into train and validation sets\n",
    "            train_data_tft = data_comb[data_comb[\"dataset_type\"] == \"train\"].copy()\n",
    "            first_val_idx_actual = data_comb[data_comb[\"dataset_type\"] == \"val\"]['time_idx'].min() # Use combined data to get correct first idx\n",
    "            start_val_idx_with_overlap = max(0, first_val_idx_actual - ENCODER_LENGTH)\n",
    "            val_data_tft_with_overlap = data_comb[data_comb[\"time_idx\"] >= start_val_idx_with_overlap].copy()\n",
    "\n",
    "\n",
    "            # --- Create TimeSeriesDataSet ---\n",
    "            print(\"Creating TimeSeriesDataSet...\")\n",
    "            target_normalizer = GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\", center=True)\n",
    "\n",
    "            training_dataset = TimeSeriesDataSet(\n",
    "                train_data_tft,\n",
    "                time_idx=\"time_idx\", target=PRIMARY_TARGET, group_ids=[\"group_id\"],\n",
    "                max_encoder_length=ENCODER_LENGTH, max_prediction_length=PREDICTION_LENGTH,\n",
    "                static_categoricals=static_categoricals,\n",
    "                time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "                time_varying_known_reals=time_varying_known_reals,\n",
    "                time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "                target_normalizer=target_normalizer,\n",
    "                add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True,\n",
    "                allow_missing_timesteps=True # <-- Fix from previous error\n",
    "            )\n",
    "\n",
    "            # Create validation dataset using the same parameters\n",
    "            validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "                training_dataset, val_data_tft_with_overlap, # Use the overlap data\n",
    "                predict=False, # Set to False for calculating validation loss\n",
    "                stop_randomization=True,\n",
    "            )\n",
    "\n",
    "            # Create dataloaders\n",
    "            train_dataloader = training_dataset.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKER)\n",
    "            val_dataloader = validation_dataset.to_dataloader(train=False, batch_size=BATCH_SIZE * 2, num_workers=NUM_WORKER)\n",
    "            print(\"TimeSeriesDataSet and Dataloaders created.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating TimeSeriesDataSet or Dataloaders: {e}\")\n",
    "            traceback.print_exc(); train_dataloader = val_dataloader = None # Prevent proceeding\n",
    "\n",
    "    # Proceed only if dataloaders created\n",
    "    if 'train_dataloader' in locals() and 'val_dataloader' in locals() and train_dataloader is not None and val_dataloader is not None:\n",
    "        # --- 5. Define Model & Trainer ---\n",
    "        print(\"\\nDefining TFT model and Trainer...\")\n",
    "        try:\n",
    "            early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
    "            lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "            # logger = TensorBoardLogger(\"tb_logs\", name=\"tft_nashik_wheat\") # Optional\n",
    "\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=MAX_EPOCHS, \n",
    "                # gpus=TRAINER_GPUS, \n",
    "                gradient_clip_val=0.1,\n",
    "                accelerator=\"gpu\",\n",
    "                devices=1, # e.g., devices=1 for one GPU\n",
    "                # gradient_clip_val=0.1,\n",
    "                # limit_train_batches=30, limit_val_batches=10, # Uncomment for debugging\n",
    "                # fast_dev_run=True, # Uncomment for quick test run\n",
    "                callbacks=[lr_monitor, early_stop_callback],\n",
    "                # logger=logger,\n",
    "                enable_progress_bar=True, enable_checkpointing=True # Ensure checkpoints are saved\n",
    "            )\n",
    "\n",
    "            tft = TemporalFusionTransformer.from_dataset(\n",
    "                training_dataset, learning_rate=LEARNING_RATE, hidden_size=HIDDEN_SIZE,\n",
    "                attention_head_size=ATTENTION_HEADS, dropout=DROPOUT, hidden_continuous_size=HIDDEN_SIZE // 2,\n",
    "                output_size=7, loss=QuantileLoss(), # Use QuantileLoss\n",
    "                # log_interval=10, # Log less frequently\n",
    "                # reduce_on_plateau_patience=4\n",
    "            )\n",
    "            print(f\"TFT Network parameters: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "            # --- 6. Train Model ---\n",
    "            print(\"\\nStarting model training...\")\n",
    "            trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "            print(\"Training finished.\")\n",
    "\n",
    "            # --- 7. Evaluate on Validation Set ---\n",
    "            print(f\"\\n--- Evaluating FINAL TFT Model Performance on {VALIDATION_YEAR} Validation Data ---\")\n",
    "            best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "            if best_model_path and os.path.exists(best_model_path):\n",
    "                 print(f\"Loading best model from: {best_model_path}\")\n",
    "                 best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "            else:\n",
    "                 print(\"Warning: Best model checkpoint not found. Using model state from end of training.\")\n",
    "                 best_tft = tft # Use the model from the last epoch if checkpoint missing\n",
    "\n",
    "            # Predict on validation data (ensure using the correct dataloader)\n",
    "            print(\"Predicting on validation data...\")\n",
    "            # Use the validation dataloader created earlier\n",
    "            raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_index=True)\n",
    "            # raw_predictions[\"prediction\"] gives quantile forecasts\n",
    "            # raw_predictions[\"index\"] gives time_idx and group_id\n",
    "\n",
    "            # Extract actuals corresponding to the predictions\n",
    "            actuals_df_eval = val_data_tft_with_overlap[lambda x: x.time_idx.isin(raw_predictions.index.time_idx)][[DATE_COLUMN, 'time_idx', 'group_id', PRIMARY_TARGET]].copy()\n",
    "\n",
    "            # Extract median prediction (quantile 0.5, which is index 3 for 7 quantiles)\n",
    "            predictions_df_eval = raw_predictions.index.copy()\n",
    "            predictions_df_eval['prediction_normalized'] = raw_predictions.prediction[:, :, 3] # Index 3 is median\n",
    "\n",
    "            # Merge actuals and normalized predictions based on time_idx and group_id\n",
    "            eval_results = pd.merge(\n",
    "                 actuals_df_eval, predictions_df_eval, on=['time_idx', 'group_id'], how='inner'\n",
    "            )\n",
    "\n",
    "            # Inverse transform using the fitted normalizer from training_dataset\n",
    "            target_scaler = training_dataset.target_normalizer\n",
    "\n",
    "            # Need to structure data for inverse_transform (often needs group_id)\n",
    "            actuals_inv = target_scaler.inverse_transform(eval_results[[PRIMARY_TARGET, 'group_id']].rename(columns={PRIMARY_TARGET:'target'}))['target'].values\n",
    "            preds_inv = target_scaler.inverse_transform(eval_results[['prediction_normalized', 'group_id']].rename(columns={'prediction_normalized':'target'}))['target'].values\n",
    "\n",
    "            # Calculate metrics\n",
    "            if len(actuals_inv) > 0 and len(preds_inv) > 0:\n",
    "                print(f\"Evaluating based on {len(actuals_inv)} matched prediction points.\")\n",
    "                r2_val, mae_val, mse_val = calculate_metrics(actuals_inv, preds_inv)\n",
    "                print(f\"FINAL Validation R-squared (R2): {r2_val:.4f}\")\n",
    "                print(f\"FINAL Validation Mean Absolute Error (MAE): {mae_val:.2f}\")\n",
    "                print(f\"FINAL Validation Mean Squared Error (MSE): {mse_val:.2f}\")\n",
    "\n",
    "                # --- 8. Plot Validation Results ---\n",
    "                print(f\"\\n--- Plotting FINAL Validation Results for {PRIMARY_TARGET} (Actual vs. Predicted {VALIDATION_YEAR}) ---\")\n",
    "                # Prepare dataframes for plotting with inverse-transformed values\n",
    "                plot_actuals_df = eval_results[[DATE_COLUMN, 'time_idx']].copy()\n",
    "                plot_actuals_df[PRIMARY_TARGET] = actuals_inv\n",
    "                plot_preds_df = eval_results[[DATE_COLUMN, 'time_idx']].copy()\n",
    "                plot_preds_df['prediction'] = preds_inv\n",
    "\n",
    "                plot_title_val = f'TFT Validation (Nashik/Wheat): {PRIMARY_TARGET.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()} Price (Actual vs. Predicted {VALIDATION_YEAR})'\n",
    "                fig_val = plot_validation_results(plot_actuals_df, plot_preds_df, PRIMARY_TARGET, DATE_COLUMN, plot_title_val)\n",
    "                fig_val.show()\n",
    "            else: print(\"Skipping metrics and plotting: No matched actuals/predictions found.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during TFT training or evaluation: {e}\"); traceback.print_exc()\n",
    "\n",
    "    else: print(\"\\nCannot proceed: lack of data after filtering or TFT dataset creation failed.\")\n",
    "else: print(\"\\nFailed during data loading, preprocessing, or mapping lookup.\")\n",
    "\n",
    "print(\"\\nProcess finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0798656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tft_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
