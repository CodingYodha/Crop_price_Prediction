{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dd826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LSTM Forecasting & Validation ---\n",
      "--- (Nashik/Wheat: 2002-2023 Train, 2024 Validate) ---\n",
      "------------------------------\n",
      "Processing Training (2002-2023) Dataset\n",
      "------------------------------\n",
      "Loading Training (2002-2023) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv...\n",
      "Loaded 6246 rows.\n",
      "Constructing 'full_date'...\n",
      "6246 rows after date construction.\n",
      "6246 rows after ensuring price columns numeric.\n",
      "Training (2002-2023) base data loaded. 6246 rows.\n",
      "------------------------------\n",
      "Processing Validation (2024) Dataset\n",
      "------------------------------\n",
      "Loading Validation (2024) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv...\n",
      "Loaded 278 rows.\n",
      "Constructing 'full_date'...\n",
      "278 rows after date construction.\n",
      "278 rows after ensuring price columns numeric.\n",
      "Validation (2024) base data loaded. 278 rows.\n",
      "\n",
      "Selected: Maharashtra/Nashik/Wheat -> Encoded: St=6291, Di=6291, Co=6291\n",
      "\n",
      "Filtering datasets using encoded values...\n",
      "\n",
      "Preparing data for LSTM (Target: avg_modal_price)...\n",
      "Creating sequences with length 120...\n",
      "Training sequences shape: X=(6126, 120, 1), y=(6126, 1)\n",
      "Validation sequences shape: X=(158, 120, 1), y=(158, 1)\n",
      "\n",
      "Building LSTM model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,540</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │        \u001b[38;5;34m12,540\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,596</span> (49.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,596\u001b[0m (49.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,596</span> (49.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,596\u001b[0m (49.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting LSTM model training...\n",
      "Epoch 1/75\n",
      "\u001b[1m163/383\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0354"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow # Uncomment if needed\n",
    "\n",
    "# --- Standard Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler # <--- For LSTM scaling\n",
    "from IPython.display import display\n",
    "import traceback\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- Keras / TensorFlow Imports ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Suppress common warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# tf.random.set_seed(42) # Optional reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Data Paths\n",
    "DATA_PATH_TRAIN = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv\"\n",
    "DATA_PATH_VALIDATION = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv\"\n",
    "\n",
    "# Target Columns & Date Construction Columns\n",
    "TARGET_COLUMNS = ['avg_min_price', 'avg_max_price', 'avg_modal_price']\n",
    "PRIMARY_TARGET = 'avg_modal_price' # Focus on one target\n",
    "DATE_COLUMN = 'full_date'; YEAR_COL = 'year'; MONTH_COL = 'month'; DAY_COL = 'date'\n",
    "VALIDATION_YEAR = 2024\n",
    "\n",
    "# Filter Selections\n",
    "SELECTED_STATE_STR = \"Maharashtra\"; SELECTED_DISTRICT_STR = \"Nashik\"; SELECTED_COMMODITY_STR = \"Wheat\"\n",
    "\n",
    "# Frequency Encoding Maps (CONFIRM THESE ARE CORRECT)\n",
    "state_name_encoding_map = {\"maharashtra\": 6291}\n",
    "district_name_encoding_map = {\"nashik\": 6291}\n",
    "commodity_name_encoding_map = {\"wheat\": 6291}\n",
    "\n",
    "# --- LSTM Configuration ---\n",
    "SEQUENCE_LENGTH = 120     # Number of past days to use for predicting the next day - NEEDS TUNING\n",
    "LSTM_UNITS = 55          # Number of units in LSTM layer - NEEDS TUNING\n",
    "DROPOUT_RATE = 0.2       # Dropout for regularization\n",
    "EPOCHS = 75              # Max epochs (use EarlyStopping) - NEEDS TUNING\n",
    "BATCH_SIZE = 16          # Batch size for training - NEEDS TUNING\n",
    "\n",
    "# --- Helper Functions (Outlier removal, Loading - Minor changes needed) ---\n",
    "def remove_outliers_iqr(df, columns_to_check):\n",
    "    df_filtered = df.copy(); initial_rows = len(df_filtered)\n",
    "    valid_columns = [col for col in columns_to_check if col in df_filtered.columns and pd.api.types.is_numeric_dtype(df_filtered[col])]\n",
    "    if not valid_columns: return df_filtered\n",
    "    subset_for_iqr = df_filtered[valid_columns]\n",
    "    Q1 = subset_for_iqr.quantile(0.25); Q3 = subset_for_iqr.quantile(0.75); IQR = Q3 - Q1\n",
    "    mask = ~((subset_for_iqr < (Q1 - 1.5 * IQR)) | (subset_for_iqr > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    df_filtered = df_filtered[mask]; rows_removed = initial_rows - len(df_filtered)\n",
    "    if rows_removed > 0: print(f\"Removed {rows_removed} rows via IQR.\")\n",
    "    return df_filtered\n",
    "\n",
    "# --- Data Loading and Preprocessing Function (Simplified for LSTM base) ---\n",
    "def load_and_preprocess_base_data(path, date_col_name, year_col, month_col, day_col, all_potential_targets, dataset_name=\"Training\"):\n",
    "    \"\"\"Loads data, constructs date, basic cleaning. Returns essential cols.\"\"\"\n",
    "    print(\"-\" * 30); print(f\"Processing {dataset_name} Dataset\"); print(\"-\" * 30)\n",
    "    try:\n",
    "        print(f\"Loading {dataset_name} data from {path}...\"); df = pd.read_csv(path); print(f\"Loaded {len(df)} rows.\")\n",
    "        # 1. Construct Date\n",
    "        date_components_cols = [year_col, month_col, day_col]\n",
    "        if not all(col in df.columns for col in date_components_cols): print(f\"Error: Date component cols missing: {[c for c in date_components_cols if c not in df.columns]}\"); return None\n",
    "        for col in date_components_cols: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=date_components_cols, inplace=True)\n",
    "        print(f\"Constructing '{date_col_name}'...\");\n",
    "        df[date_col_name] = pd.to_datetime({'year': df[year_col], 'month': df[month_col], 'day': df[day_col]}, errors='coerce')\n",
    "        initial_rows_date = len(df); df.dropna(subset=[date_col_name], inplace=True)\n",
    "        if initial_rows_date > len(df): print(f\"Dropped {initial_rows_date - len(df)} rows due to invalid date components.\")\n",
    "        print(f\"{len(df)} rows after date construction.\")\n",
    "\n",
    "        # 2. Keep ONLY necessary columns for filtering + target + date\n",
    "        required_numeric_filter_cols = ['state_name', 'district_name', 'commodity_name']\n",
    "        keep_cols = [date_col_name] + all_potential_targets + required_numeric_filter_cols\n",
    "        missing_req_cols = [col for col in keep_cols if col not in df.columns]\n",
    "        if missing_req_cols: print(f\"Error: Required columns missing: {missing_req_cols}\"); print(f\"Available: {df.columns.tolist()}\"); return None\n",
    "        df = df[keep_cols] # Keep only needed columns early on\n",
    "\n",
    "        # 3. Ensure Price/Target columns are numeric\n",
    "        for col in all_potential_targets: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=all_potential_targets, how='any', inplace=True)\n",
    "        print(f\"{len(df)} rows after ensuring price columns numeric.\")\n",
    "\n",
    "        # 4. Ensure Filter columns are numeric (encoded)\n",
    "        for col in required_numeric_filter_cols:\n",
    "             if not pd.api.types.is_numeric_dtype(df[col]): print(f\"Error: Col '{col}' expected numeric but isn't.\"); return None\n",
    "\n",
    "        # 5. Apply IQR Outlier Removal (Optional, on targets)\n",
    "        # df = remove_outliers_iqr(df, all_potential_targets)\n",
    "\n",
    "        df.sort_values(date_col_name, inplace=True)\n",
    "        print(f\"{dataset_name} base data loaded. {len(df)} rows.\")\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError: print(f\"Error: {dataset_name} file not found at {path}\"); return None\n",
    "    except Exception as e: print(f\"Error loading/preprocessing {dataset_name}: {e}\"); traceback.print_exc(); return None\n",
    "\n",
    "\n",
    "# --- Sequence Creation Function ---\n",
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"Creates sequences of data for LSTM.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length)]) # Sequence of inputs\n",
    "        y.append(data[i + sequence_length])    # Value to predict\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- Evaluation Metrics Function (No changes) ---\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # ... (same as before) ...\n",
    "    y_true = np.array(y_true).flatten(); y_pred = np.array(y_pred).flatten()\n",
    "    valid_mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true = y_true[valid_mask]; y_pred = y_pred[valid_mask]\n",
    "    if len(y_true) == 0: print(\"Warning: No valid points for metric calculation.\"); return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred); mae = mean_absolute_error(y_true, y_pred); mse = mean_squared_error(y_true, y_pred)\n",
    "        return r2, mae, mse\n",
    "    except Exception as e: print(f\"Error calculating metrics: {e}\"); return np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "# --- Plotting Function for Validation (Adapted for LSTM Output) ---\n",
    "def plot_lstm_validation_results(dates_val, actuals_inv, preds_inv, target_column, title):\n",
    "    \"\"\"Plots actuals vs predictions for validation period.\"\"\"\n",
    "    import plotly.graph_objects as go # Import locally if not globally\n",
    "    fig = go.Figure(); target_label = target_column.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()\n",
    "\n",
    "    # Actuals\n",
    "    fig.add_trace(go.Scatter(x=dates_val, y=actuals_inv.flatten(), mode='lines+markers', name=f'Actual {target_label} ({VALIDATION_YEAR})', line=dict(color='blue'), marker=dict(size=4)))\n",
    "\n",
    "    # Predictions\n",
    "    fig.add_trace(go.Scatter(x=dates_val, y=preds_inv.flatten(), mode='lines', name=f'Predicted {target_label} ({VALIDATION_YEAR})', line=dict(color='red')))\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=f'Date ({VALIDATION_YEAR})', yaxis_title=f'Price ({target_label})', hovermode=\"x unified\", legend_title_text='Legend')\n",
    "    return fig\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(\"--- LSTM Forecasting & Validation ---\") # Changed Title\n",
    "print(f\"--- (Nashik/Wheat: 2002-2023 Train, {VALIDATION_YEAR} Validate) ---\")\n",
    "\n",
    "# 1. Load Base Data\n",
    "df_train_base = load_and_preprocess_base_data(DATA_PATH_TRAIN, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, \"Training (2002-2023)\")\n",
    "df_val_base = load_and_preprocess_base_data(DATA_PATH_VALIDATION, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, f\"Validation ({VALIDATION_YEAR})\")\n",
    "\n",
    "# Init flags/variables\n",
    "model = None\n",
    "\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "    # 2. Get Encoded Values for Filtering\n",
    "    try:\n",
    "        selected_state_key=SELECTED_STATE_STR.strip().lower(); selected_district_key=SELECTED_DISTRICT_STR.strip().lower(); selected_commodity_key=SELECTED_COMMODITY_STR.strip().lower()\n",
    "        encoded_state = state_name_encoding_map.get(selected_state_key); encoded_district = district_name_encoding_map.get(selected_district_key); encoded_commodity = commodity_name_encoding_map.get(selected_commodity_key)\n",
    "        lookup_failed = False\n",
    "        if encoded_state is None: print(f\"Error: State '{SELECTED_STATE_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_district is None: print(f\"Error: District '{SELECTED_DISTRICT_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_commodity is None: print(f\"Error: Commodity '{SELECTED_COMMODITY_STR}' missing map.\"); lookup_failed=True\n",
    "        if lookup_failed: print(\"Check maps.\"); df_train_base=df_val_base=None\n",
    "        else: print(f\"\\nSelected: {SELECTED_STATE_STR}/{SELECTED_DISTRICT_STR}/{SELECTED_COMMODITY_STR} -> Encoded: St={encoded_state}, Di={encoded_district}, Co={encoded_commodity}\")\n",
    "    except Exception as e: print(f\"Error mapping lookup: {e}\"); df_train_base = df_val_base = None\n",
    "\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "    # 3. Filtering Data\n",
    "    print(f\"\\nFiltering datasets using encoded values...\")\n",
    "    filter_cols_num = ['state_name', 'district_name', 'commodity_name']\n",
    "    if not all(col in df_train_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Training.\"); filtered_df_train = pd.DataFrame()\n",
    "    else: filtered_df_train = df_train_base[(df_train_base['state_name'] == encoded_state) & (df_train_base['district_name'] == encoded_district) & (df_train_base['commodity_name'] == encoded_commodity)].copy(); filtered_df_train.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "    if not all(col in df_val_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Validation.\"); filtered_df_val = pd.DataFrame()\n",
    "    else: filtered_df_val = df_val_base[(df_val_base['state_name'] == encoded_state) & (df_val_base['district_name'] == encoded_district) & (df_val_base['commodity_name'] == encoded_commodity)].copy(); filtered_df_val.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "\n",
    "    if filtered_df_train.empty: print(\"\\nWarning: No training data after filtering.\")\n",
    "    if filtered_df_val.empty: print(\"\\nWarning: No validation data after filtering.\")\n",
    "\n",
    "    # 4. Prepare Data for LSTM (Focusing on PRIMARY_TARGET)\n",
    "    if not filtered_df_train.empty and not filtered_df_val.empty:\n",
    "        print(f\"\\nPreparing data for LSTM (Target: {PRIMARY_TARGET})...\")\n",
    "\n",
    "        # Select target series and ensure correct shape (n_samples, 1)\n",
    "        train_series = filtered_df_train[PRIMARY_TARGET].values.reshape(-1, 1)\n",
    "        val_series = filtered_df_val[PRIMARY_TARGET].values.reshape(-1, 1)\n",
    "        val_dates = filtered_df_val[DATE_COLUMN].values # Keep dates for plotting\n",
    "\n",
    "        # Scale data (Fit only on training data)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_train_data = scaler.fit_transform(train_series)\n",
    "        scaled_val_data = scaler.transform(val_series) # Use same scaler\n",
    "\n",
    "        # Create sequences\n",
    "        print(f\"Creating sequences with length {SEQUENCE_LENGTH}...\")\n",
    "        X_train, y_train = create_sequences(scaled_train_data, SEQUENCE_LENGTH)\n",
    "        X_val, y_val = create_sequences(scaled_val_data, SEQUENCE_LENGTH)\n",
    "\n",
    "        # Validation dates need adjustment: remove first `SEQUENCE_LENGTH` dates\n",
    "        # as they don't have a corresponding prediction in this simple setup.\n",
    "        dates_val_for_plotting = val_dates[SEQUENCE_LENGTH:]\n",
    "        # Actual validation values corresponding to predictions\n",
    "        y_val_actual_unscaled = val_series[SEQUENCE_LENGTH:] # Unscaled actuals\n",
    "\n",
    "        if X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
    "             print(\"Error: Not enough data to create sequences for training or validation after filtering.\")\n",
    "             model = None # Prevent proceeding\n",
    "        else:\n",
    "             print(f\"Training sequences shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "             print(f\"Validation sequences shape: X={X_val.shape}, y={y_val.shape}\")\n",
    "\n",
    "             # --- 5. Build LSTM Model ---\n",
    "             print(\"\\nBuilding LSTM model...\")\n",
    "             model = Sequential()\n",
    "             model.add(LSTM(LSTM_UNITS, activation='relu', # Or 'tanh'\n",
    "                            input_shape=(SEQUENCE_LENGTH, 1))) # Input: sequence_length time steps, 1 feature\n",
    "             model.add(Dropout(DROPOUT_RATE))\n",
    "             # Add more LSTM layers if needed:\n",
    "             # model.add(LSTM(LSTM_UNITS // 2, activation='relu', return_sequences=True))\n",
    "             # model.add(Dropout(DROPOUT_RATE))\n",
    "             model.add(Dense(1)) # Output layer: predicts 1 value\n",
    "\n",
    "             model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "             model.summary()\n",
    "\n",
    "             # --- 6. Train LSTM Model ---\n",
    "             print(\"\\nStarting LSTM model training...\")\n",
    "             early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "             history = model.fit(\n",
    "                 X_train, y_train,\n",
    "                 epochs=EPOCHS,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 validation_data=(X_val, y_val),\n",
    "                 callbacks=[early_stopping],\n",
    "                 verbose=1 # Set to 1 or 2 for progress, 0 for silent\n",
    "             )\n",
    "             print(\"Training finished.\")\n",
    "\n",
    "    # --- 7. Evaluate on Validation Set ---\n",
    "    if model is not None and 'X_val' in locals() and X_val.shape[0] > 0:\n",
    "        print(f\"\\n--- Evaluating FINAL LSTM Model Performance on {VALIDATION_YEAR} Validation Data ---\")\n",
    "        print(\"Predicting on validation data...\")\n",
    "        predictions_scaled = model.predict(X_val)\n",
    "\n",
    "        # Inverse transform predictions and actuals\n",
    "        print(\"Inverse transforming scaled predictions and actuals...\")\n",
    "        try:\n",
    "             predictions_inv = scaler.inverse_transform(predictions_scaled)\n",
    "             # y_val_actual_unscaled was stored earlier\n",
    "             actuals_inv = y_val_actual_unscaled\n",
    "\n",
    "             # Ensure shapes match for metric calculation\n",
    "             min_len_eval = min(len(actuals_inv), len(predictions_inv))\n",
    "             if len(actuals_inv) != len(predictions_inv):\n",
    "                 print(f\"Warning: Length mismatch after prediction/scaling. Actuals: {len(actuals_inv)}, Preds: {len(predictions_inv)}. Truncating.\")\n",
    "             actuals_inv = actuals_inv[:min_len_eval]\n",
    "             predictions_inv = predictions_inv[:min_len_eval]\n",
    "             dates_val_for_plotting = dates_val_for_plotting[:min_len_eval] # Adjust dates too\n",
    "\n",
    "             # Calculate metrics\n",
    "             if len(actuals_inv) > 0:\n",
    "                 r2_val, mae_val, mse_val = calculate_metrics(actuals_inv, predictions_inv)\n",
    "                 print(f\"FINAL Validation R-squared (R2): {r2_val:.4f}\")\n",
    "                 print(f\"FINAL Validation Mean Absolute Error (MAE): {mae_val:.2f}\")\n",
    "                 print(f\"FINAL Validation Mean Squared Error (MSE): {mse_val:.2f}\")\n",
    "\n",
    "                 # --- 8. Plot Validation Results ---\n",
    "                 print(f\"\\n--- Plotting FINAL Validation Results for {PRIMARY_TARGET} (Actual vs. Predicted {VALIDATION_YEAR}) ---\")\n",
    "                 plot_title_val = f'LSTM Validation (Nashik/Wheat): {PRIMARY_TARGET.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()} Price (Actual vs. Predicted {VALIDATION_YEAR})'\n",
    "                 fig_val = plot_lstm_validation_results(dates_val_for_plotting, actuals_inv, predictions_inv, PRIMARY_TARGET, plot_title_val)\n",
    "                 fig_val.show()\n",
    "             else:\n",
    "                 print(\"Skipping metrics and plotting: No valid data points after alignment/scaling.\")\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Error during prediction, scaling or evaluation: {e}\")\n",
    "             traceback.print_exc()\n",
    "\n",
    "    elif model is None:\n",
    "         print(\"\\nSkipping evaluation and plotting because model training failed or insufficient data.\")\n",
    "    else: # Handle cases where filtering worked but sequence creation failed\n",
    "         print(\"\\nCannot proceed: lack of data after filtering or sequence creation.\")\n",
    "else:\n",
    "    print(\"\\nFailed during data loading, preprocessing, or mapping lookup.\")\n",
    "\n",
    "print(\"\\nProcess finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acff607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
