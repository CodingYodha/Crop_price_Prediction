{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"E:\\elevatetrsest\\dataset\\price_wheat_daily.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>district_id</th>\n",
       "      <th>district_name</th>\n",
       "      <th>commodity_name</th>\n",
       "      <th>state_name</th>\n",
       "      <th>avg_modal_price</th>\n",
       "      <th>avg_min_price</th>\n",
       "      <th>avg_max_price</th>\n",
       "      <th>calculationType</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>497</td>\n",
       "      <td>Nandurbar</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2673.0</td>\n",
       "      <td>2525.5</td>\n",
       "      <td>2716.00</td>\n",
       "      <td>Daily</td>\n",
       "      <td>100.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-06</td>\n",
       "      <td>497</td>\n",
       "      <td>Nandurbar</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2572.5</td>\n",
       "      <td>2470.0</td>\n",
       "      <td>2710.50</td>\n",
       "      <td>Daily</td>\n",
       "      <td>-61.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-05</td>\n",
       "      <td>497</td>\n",
       "      <td>Nandurbar</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2634.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2740.00</td>\n",
       "      <td>Daily</td>\n",
       "      <td>147.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>497</td>\n",
       "      <td>Nandurbar</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2486.5</td>\n",
       "      <td>2390.5</td>\n",
       "      <td>2628.75</td>\n",
       "      <td>Daily</td>\n",
       "      <td>-238.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-03</td>\n",
       "      <td>497</td>\n",
       "      <td>Nandurbar</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2725.0</td>\n",
       "      <td>2492.0</td>\n",
       "      <td>2778.00</td>\n",
       "      <td>Daily</td>\n",
       "      <td>-31.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  district_id district_name commodity_name   state_name  \\\n",
       "0  2025-03-07          497     Nandurbar          Wheat  Maharashtra   \n",
       "1  2025-03-06          497     Nandurbar          Wheat  Maharashtra   \n",
       "2  2025-03-05          497     Nandurbar          Wheat  Maharashtra   \n",
       "3  2025-03-04          497     Nandurbar          Wheat  Maharashtra   \n",
       "4  2025-03-03          497     Nandurbar          Wheat  Maharashtra   \n",
       "\n",
       "   avg_modal_price  avg_min_price  avg_max_price calculationType  change  \n",
       "0           2673.0         2525.5        2716.00           Daily   100.5  \n",
       "1           2572.5         2470.0        2710.50           Daily   -61.5  \n",
       "2           2634.0         2500.0        2740.00           Daily   147.5  \n",
       "3           2486.5         2390.5        2628.75           Daily  -238.5  \n",
       "4           2725.0         2492.0        2778.00           Daily   -31.5  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30501 entries, 0 to 30500\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   date             30501 non-null  object \n",
      " 1   district_id      30501 non-null  int64  \n",
      " 2   district_name    30501 non-null  object \n",
      " 3   commodity_name   30501 non-null  object \n",
      " 4   state_name       30501 non-null  object \n",
      " 5   avg_modal_price  30501 non-null  float64\n",
      " 6   avg_min_price    30501 non-null  float64\n",
      " 7   avg_max_price    30501 non-null  float64\n",
      " 8   calculationType  30501 non-null  object \n",
      " 9   change           30470 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(5)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['date_n'] = pd.to_datetime(df['date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "df['date'] = df['date_n']\n",
    "df.drop('date_n', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_541760\\2572123431.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'interpolate' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna('interpolate' , inplace=True)\n"
     ]
    }
   ],
   "source": [
    " df.fillna('interpolate' , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30501 entries, 0 to 30500\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   district_id      30501 non-null  int64  \n",
      " 1   district_name    30501 non-null  object \n",
      " 2   commodity_name   30501 non-null  object \n",
      " 3   state_name       30501 non-null  object \n",
      " 4   avg_modal_price  30501 non-null  float64\n",
      " 5   avg_min_price    30501 non-null  float64\n",
      " 6   avg_max_price    30501 non-null  float64\n",
      " 7   calculationType  30501 non-null  object \n",
      " 8   change           30501 non-null  object \n",
      " 9   month            30501 non-null  int32  \n",
      " 10  year             30501 non-null  int32  \n",
      " 11  date             30501 non-null  int32  \n",
      "dtypes: float64(3), int32(3), int64(1), object(5)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frequency encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['district_name' , 'commodity_name' , 'state_name']\n",
    "\n",
    "for col in columns:\n",
    "    frequency_encoding = df[col].value_counts()\n",
    "    df[f'{col}_enc'] = df[col].map(frequency_encoding)\n",
    "\n",
    "df.drop(columns=columns , axis=1 , inplace=True)\n",
    "df.drop(columns=['calculationType' , 'district_id' , 'change'] , axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_modal_price</th>\n",
       "      <th>avg_min_price</th>\n",
       "      <th>avg_max_price</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>district_name_enc</th>\n",
       "      <th>commodity_name_enc</th>\n",
       "      <th>state_name_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2673.0</td>\n",
       "      <td>2525.5</td>\n",
       "      <td>2716.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>7</td>\n",
       "      <td>981</td>\n",
       "      <td>30501</td>\n",
       "      <td>30501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2572.5</td>\n",
       "      <td>2470.0</td>\n",
       "      <td>2710.50</td>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>981</td>\n",
       "      <td>30501</td>\n",
       "      <td>30501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2634.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2740.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>981</td>\n",
       "      <td>30501</td>\n",
       "      <td>30501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2486.5</td>\n",
       "      <td>2390.5</td>\n",
       "      <td>2628.75</td>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>981</td>\n",
       "      <td>30501</td>\n",
       "      <td>30501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2725.0</td>\n",
       "      <td>2492.0</td>\n",
       "      <td>2778.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>981</td>\n",
       "      <td>30501</td>\n",
       "      <td>30501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_modal_price  avg_min_price  avg_max_price  month  year  date  \\\n",
       "0           2673.0         2525.5        2716.00      3  2025     7   \n",
       "1           2572.5         2470.0        2710.50      3  2025     6   \n",
       "2           2634.0         2500.0        2740.00      3  2025     5   \n",
       "3           2486.5         2390.5        2628.75      3  2025     4   \n",
       "4           2725.0         2492.0        2778.00      3  2025     3   \n",
       "\n",
       "   district_name_enc  commodity_name_enc  state_name_enc  \n",
       "0                981               30501           30501  \n",
       "1                981               30501           30501  \n",
       "2                981               30501           30501  \n",
       "3                981               30501           30501  \n",
       "4                981               30501           30501  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30501 entries, 0 to 30500\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   avg_modal_price     30501 non-null  float64\n",
      " 1   avg_min_price       30501 non-null  float64\n",
      " 2   avg_max_price       30501 non-null  float64\n",
      " 3   month               30501 non-null  int32  \n",
      " 4   year                30501 non-null  int32  \n",
      " 5   date                30501 non-null  int32  \n",
      " 6   district_name_enc   30501 non-null  int64  \n",
      " 7   commodity_name_enc  30501 non-null  int64  \n",
      " 8   state_name_enc      30501 non-null  int64  \n",
      "dtypes: float64(3), int32(3), int64(3)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strategy to tackle the following problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# 1. Prepare Time Series Features\n",
    "def create_time_features(df):\n",
    "    \"\"\"Create additional time-based features\"\"\"\n",
    "    # Convert date columns to datetime if needed\n",
    "    # Assuming 'date' is day of month, and we need to create a full date column\n",
    "    df['full_date'] = pd.to_datetime(df[['year', 'month', 'date']].assign(day=lambda x: x['date']))\n",
    "    \n",
    "    # Create lag features for each target\n",
    "    for target in ['avg_modal_price', 'avg_min_price', 'avg_max_price']:\n",
    "        for lag in [1, 3, 7, 14, 30]:  # Various lag periods\n",
    "            # Group by relevant features to create lags\n",
    "            df_grouped = df.groupby(['commodity_name_enc', 'state_name_enc', 'district_name_enc'])\n",
    "            df[f'{target}_lag_{lag}'] = df_grouped[target].shift(lag)\n",
    "    \n",
    "    # Create rolling statistics\n",
    "    for target in ['avg_modal_price', 'avg_min_price', 'avg_max_price']:\n",
    "        for window in [7, 14, 30]:\n",
    "            # Group by relevant features \n",
    "            df_grouped = df.groupby(['commodity_name_enc', 'state_name_enc', 'district_name_enc'])\n",
    "            df[f'{target}_rolling_mean_{window}'] = df_grouped[target].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "            df[f'{target}_rolling_std_{window}'] = df_grouped[target].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "    \n",
    "    # Create cyclical features for month\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    \n",
    "    # Create day of year feature\n",
    "    df['day_of_year'] = df['full_date'].dt.dayofyear\n",
    "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year']/365)\n",
    "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year']/365)\n",
    "    \n",
    "    # Add trend feature\n",
    "    df['trend'] = np.arange(len(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Time Series Validation Strategy\n",
    "def time_series_validation(df, model_func, features, targets, n_splits=5):\n",
    "    \"\"\"Validate model using time series cross-validation\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    results = {target: {'rmse': [], 'mae': [], 'r2': []} for target in targets}\n",
    "    \n",
    "    # Sort by date for proper time-based splitting\n",
    "    df = df.sort_values('full_date')\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[targets]\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Fill NaN values that might be created by lags\n",
    "        X_train = X_train.fillna(method='bfill')\n",
    "        X_test = X_test.fillna(method='bfill')\n",
    "        \n",
    "        # Train and evaluate model for each target\n",
    "        for target in targets:\n",
    "            model = model_func()\n",
    "            model.fit(X_train, y_train[target])\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(y_test[target], y_pred))\n",
    "            mae = mean_absolute_error(y_test[target], y_pred)\n",
    "            r2 = r2_score(y_test[target], y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            results[target]['rmse'].append(rmse)\n",
    "            results[target]['mae'].append(mae)\n",
    "            results[target]['r2'].append(r2)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    for target in targets:\n",
    "        for metric in ['rmse', 'mae', 'r2']:\n",
    "            results[target][f'avg_{metric}'] = np.mean(results[target][metric])\n",
    "            \n",
    "    return results\n",
    "\n",
    "# 3. XGBoost with Time Series Features\n",
    "def train_xgboost_time_series():\n",
    "    \"\"\"Train XGBoost model with time series considerations\"\"\"\n",
    "    # Prepare data with all features\n",
    "    processed_df = create_time_features(df)\n",
    "    \n",
    "    # Define features and targets\n",
    "    targets = ['avg_modal_price', 'avg_min_price', 'avg_max_price']\n",
    "    features = [\n",
    "        # Original features\n",
    "        'month', 'year', 'date', 'district_name_enc', 'commodity_name_enc', 'state_name_enc',\n",
    "        # Time features\n",
    "        'month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos', 'trend',\n",
    "        # Lag features (partial list)\n",
    "        'avg_modal_price_lag_1', 'avg_modal_price_lag_7', 'avg_modal_price_lag_30',\n",
    "        'avg_min_price_lag_1', 'avg_min_price_lag_7', 'avg_min_price_lag_30',\n",
    "        'avg_max_price_lag_1', 'avg_max_price_lag_7', 'avg_max_price_lag_30',\n",
    "        # Rolling statistics (partial list)\n",
    "        'avg_modal_price_rolling_mean_7', 'avg_modal_price_rolling_std_7',\n",
    "        'avg_modal_price_rolling_mean_30', 'avg_modal_price_rolling_std_30'\n",
    "    ]\n",
    "    \n",
    "    # Create training and testing datasets with a time-based split\n",
    "    train_size = int(len(processed_df) * 0.8)\n",
    "    train_df = processed_df.iloc[:train_size]\n",
    "    test_df = processed_df.iloc[train_size:]\n",
    "    \n",
    "    # Handle missing values that might arise from lag features\n",
    "    train_df = train_df.fillna(method='bfill')\n",
    "    test_df = test_df.fillna(method='bfill')\n",
    "    \n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[targets]\n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df[targets]\n",
    "    \n",
    "    # Train models for each target\n",
    "    models = {}\n",
    "    results = {}\n",
    "    feature_importances = {}\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"Training XGBoost model for {target}...\")\n",
    "        \n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            min_child_weight=1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, \n",
    "            y_train[target],\n",
    "            eval_set=[(X_test, y_test[target])],\n",
    "            early_stopping_rounds=10,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[target], y_pred))\n",
    "        mae = mean_absolute_error(y_test[target], y_pred)\n",
    "        r2 = r2_score(y_test[target], y_pred)\n",
    "        \n",
    "        print(f\"{target} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        # Store model and results\n",
    "        models[target] = model\n",
    "        results[target] = {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "        \n",
    "        # Get feature importance\n",
    "        feature_importances[target] = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 important features:\")\n",
    "        print(feature_importances[target].head(10))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return models, results, feature_importances\n",
    "\n",
    "# 4. Prophet/SARIMA for Specific Commodity-Location Combinations\n",
    "def train_sarima_for_specific_combination(commodity_id, state_id, district_id):\n",
    "    \"\"\"Train SARIMA model for specific commodity and location\"\"\"\n",
    "    # Filter data for specific combination\n",
    "    filter_mask = (\n",
    "        (df['commodity_name_enc'] == commodity_id) & \n",
    "        (df['state_name_enc'] == state_id) & \n",
    "        (df['district_name_enc'] == district_id)\n",
    "    )\n",
    "    \n",
    "    specific_df = df[filter_mask].copy()\n",
    "    \n",
    "    # Create datetime index\n",
    "    specific_df['date'] = pd.to_datetime(specific_df[['year', 'month', 'date']].assign(day=lambda x: x['date']))\n",
    "    specific_df = specific_df.set_index('date')\n",
    "    specific_df = specific_df.sort_index()\n",
    "    \n",
    "    # Focus on single target for SARIMA\n",
    "    target_series = specific_df['avg_modal_price']\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(target_series) * 0.8)\n",
    "    train_series = target_series[:train_size]\n",
    "    test_series = target_series[train_size:]\n",
    "    \n",
    "    # Fit SARIMA model\n",
    "    # p, d, q represent AR terms, differencing, and MA terms\n",
    "    # P, D, Q represent seasonal components\n",
    "    # s represents seasonal periodicity (12 for monthly, 30 for daily)\n",
    "    model = SARIMAX(\n",
    "        train_series,\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(1, 1, 1, 30),\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    \n",
    "    fitted_model = model.fit(disp=False)\n",
    "    \n",
    "    # Forecast\n",
    "    forecast = fitted_model.get_forecast(steps=len(test_series))\n",
    "    forecast_mean = forecast.predicted_mean\n",
    "    forecast_ci = forecast.conf_int()\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(test_series, forecast_mean))\n",
    "    mae = mean_absolute_error(test_series, forecast_mean)\n",
    "    \n",
    "    print(f\"SARIMA - RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_series.index, train_series, label='Training Data')\n",
    "    plt.plot(test_series.index, test_series, label='Actual Test Data')\n",
    "    plt.plot(test_series.index, forecast_mean, label='SARIMA Forecast')\n",
    "    plt.fill_between(\n",
    "        test_series.index,\n",
    "        forecast_ci.iloc[:, 0],\n",
    "        forecast_ci.iloc[:, 1],\n",
    "        color='k', alpha=0.1\n",
    "    )\n",
    "    plt.title(f'SARIMA Forecast for Commodity {commodity_id} in District {district_id}, State {state_id}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fitted_model, forecast_mean, rmse, mae\n",
    "\n",
    "# 5. Neural Networks for Sequence Forecasting (Example with TensorFlow)\n",
    "def build_lstm_model(train_df, test_df, targets, lookback=30):\n",
    "    \"\"\"\n",
    "    Example of using LSTM for time series forecasting\n",
    "    Note: This requires TensorFlow to be installed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "    except ImportError:\n",
    "        print(\"TensorFlow not installed. Skip LSTM implementation.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Prepare sequences for LSTM\n",
    "    def create_sequences(data, target_col, lookback):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - lookback):\n",
    "            X.append(data.iloc[i:i+lookback].values)\n",
    "            y.append(data.iloc[i+lookback][target_col])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Example for one target\n",
    "    target = targets[0]  # Using first target for example\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(train_df[['avg_modal_price', 'avg_min_price', 'avg_max_price']]),\n",
    "        columns=['avg_modal_price', 'avg_min_price', 'avg_max_price']\n",
    "    )\n",
    "    test_scaled = pd.DataFrame(\n",
    "        scaler.transform(test_df[['avg_modal_price', 'avg_min_price', 'avg_max_price']]),\n",
    "        columns=['avg_modal_price', 'avg_min_price', 'avg_max_price']\n",
    "    )\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_scaled, target, lookback)\n",
    "    X_test, y_test = create_sequences(test_scaled, target, lookback)\n",
    "    \n",
    "    # Reshape for LSTM [samples, timesteps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Build LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(lookback, X_train.shape[2])),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform for evaluation\n",
    "    y_test_original = y_test * scaler.scale_[0] + scaler.mean_[0]\n",
    "    y_pred_original = y_pred * scaler.scale_[0] + scaler.mean_[0]\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "    mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    \n",
    "    print(f\"LSTM for {target} - RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# 6. Ensemble Method\n",
    "def train_ensemble_model(df, features, targets):\n",
    "    \"\"\"\n",
    "    Train an ensemble of different models and combine their predictions\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    processed_df = create_time_features(df)\n",
    "    train_size = int(len(processed_df) * 0.8)\n",
    "    train_df = processed_df.iloc[:train_size]\n",
    "    test_df = processed_df.iloc[train_size:]\n",
    "    \n",
    "    # Handle missing values\n",
    "    train_df = train_df.fillna(method='bfill')\n",
    "    test_df = test_df.fillna(method='bfill')\n",
    "    \n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[targets]\n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df[targets]\n",
    "    \n",
    "    # Dictionary to store individual models\n",
    "    model_predictions = {target: {} for target in targets}\n",
    "    ensemble_results = {target: {} for target in targets}\n",
    "    \n",
    "    for target in targets:\n",
    "        print(f\"Training ensemble for {target}...\")\n",
    "        \n",
    "        # Model 1: XGBoost\n",
    "        xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "        xgb_model.fit(X_train, y_train[target])\n",
    "        xgb_pred = xgb_model.predict(X_test)\n",
    "        model_predictions[target]['xgb'] = xgb_pred\n",
    "        \n",
    "        # Model 2: Random Forest (from sklearn)\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf_model.fit(X_train, y_train[target])\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        model_predictions[target]['rf'] = rf_pred\n",
    "        \n",
    "        # Model 3: LightGBM (if available)\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "            lgb_model.fit(X_train, y_train[target])\n",
    "            lgb_pred = lgb_model.predict(X_test)\n",
    "            model_predictions[target]['lgb'] = lgb_pred\n",
    "        except ImportError:\n",
    "            print(\"LightGBM not installed. Skipping this model.\")\n",
    "        \n",
    "        # Simple average ensemble\n",
    "        ensemble_preds = np.mean([pred for pred in model_predictions[target].values()], axis=0)\n",
    "        \n",
    "        # Evaluate ensemble\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[target], ensemble_preds))\n",
    "        mae = mean_absolute_error(y_test[target], ensemble_preds)\n",
    "        r2 = r2_score(y_test[target], ensemble_preds)\n",
    "        \n",
    "        ensemble_results[target] = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'predictions': ensemble_preds\n",
    "        }\n",
    "        \n",
    "        print(f\"Ensemble for {target} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        # Compare with individual models\n",
    "        print(\"Individual model performance:\")\n",
    "        for model_name, preds in model_predictions[target].items():\n",
    "            model_rmse = np.sqrt(mean_squared_error(y_test[target], preds))\n",
    "            model_r2 = r2_score(y_test[target], preds)\n",
    "            print(f\"  {model_name} - RMSE: {model_rmse:.4f}, R²: {model_r2:.4f}\")\n",
    "    \n",
    "    return ensemble_results, model_predictions\n",
    "\n",
    "# 7. Feature Importance Analysis\n",
    "def analyze_feature_importance(model, features):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance from a trained model\n",
    "    \"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    # Create dataframe for importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importance['Feature'][:15], feature_importance['Importance'][:15])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Top 15 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# 8. Explainability with SHAP\n",
    "def shap_analysis(model, X_test):\n",
    "    \"\"\"\n",
    "    Perform SHAP analysis to explain model predictions\n",
    "    Note: Requires SHAP package\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import shap\n",
    "        \n",
    "        # Initialize SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "        # Summary plot\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "        \n",
    "        # Dependence plots for top features\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': X_test.columns,\n",
    "            'Importance': np.abs(shap_values).mean(0)\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        top_features = feature_importance['Feature'][:3].values\n",
    "        \n",
    "        for feature in top_features:\n",
    "            shap.dependence_plot(feature, shap_values, X_test)\n",
    "            \n",
    "        return shap_values, explainer\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"SHAP not installed. Skip SHAP analysis.\")\n",
    "        return None, None\n",
    "\n",
    "# 9. Forecast Future Prices\n",
    "def forecast_future_prices(model, last_data, features, future_days=30):\n",
    "    \"\"\"\n",
    "    Forecast future prices based on the latest data\n",
    "    \"\"\"\n",
    "    # Create future dates\n",
    "    last_date = pd.to_datetime(last_data[['year', 'month', 'date']].iloc[-1].assign(day=lambda x: x['date']))\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=future_days)\n",
    "    \n",
    "    # Create dataframe for future data\n",
    "    future_df = pd.DataFrame({\n",
    "        'full_date': future_dates,\n",
    "        'year': future_dates.year,\n",
    "        'month': future_dates.month,\n",
    "        'date': future_dates.day\n",
    "    })\n",
    "    \n",
    "    # Add constant features from last known data\n",
    "    for col in ['district_name_enc', 'commodity_name_enc', 'state_name_enc']:\n",
    "        future_df[col] = last_data[col].iloc[-1]\n",
    "    \n",
    "    # Add time features\n",
    "    future_df['month_sin'] = np.sin(2 * np.pi * future_df['month']/12)\n",
    "    future_df['month_cos'] = np.cos(2 * np.pi * future_df['month']/12)\n",
    "    future_df['day_of_year'] = future_df['full_date'].dt.dayofyear\n",
    "    future_df['day_of_year_sin'] = np.sin(2 * np.pi * future_df['day_of_year']/365)\n",
    "    future_df['day_of_year_cos'] = np.cos(2 * np.pi * future_df['day_of_year']/365)\n",
    "    \n",
    "    # Add trend\n",
    "    future_df['trend'] = np.arange(len(last_data), len(last_data) + len(future_df))\n",
    "    \n",
    "    # Initialize predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # For each day, predict and update lag features\n",
    "    current_df = last_data.copy().tail(30)  # Use last 30 days to calculate initial lags\n",
    "    \n",
    "    for i in range(future_days):\n",
    "        # Current day to predict\n",
    "        current_day = future_df.iloc[i:i+1].copy()\n",
    "        \n",
    "        # Calculate lag features based on previous predictions and historical data\n",
    "        for lag in [1, 3, 7, 14, 30]:\n",
    "            if lag <= i:\n",
    "                # Use previous predictions\n",
    "                pred_idx = i - lag\n",
    "                current_day['avg_modal_price_lag_{}'.format(lag)] = predictions[pred_idx]['avg_modal_price']\n",
    "                current_day['avg_min_price_lag_{}'.format(lag)] = predictions[pred_idx]['avg_min_price']\n",
    "                current_day['avg_max_price_lag_{}'.format(lag)] = predictions[pred_idx]['avg_max_price']\n",
    "            else:\n",
    "                # Use historical data\n",
    "                hist_idx = -lag + i\n",
    "                if hist_idx < 0 and abs(hist_idx) <= len(current_df):\n",
    "                    current_day['avg_modal_price_lag_{}'.format(lag)] = current_df['avg_modal_price'].iloc[hist_idx]\n",
    "                    current_day['avg_min_price_lag_{}'.format(lag)] = current_df['avg_min_price'].iloc[hist_idx]\n",
    "                    current_day['avg_max_price_lag_{}'.format(lag)] = current_df['avg_max_price'].iloc[hist_idx]\n",
    "                else:\n",
    "                    # Fallback if we don't have enough history\n",
    "                    current_day['avg_modal_price_lag_{}'.format(lag)] = current_df['avg_modal_price'].mean()\n",
    "                    current_day['avg_min_price_lag_{}'.format(lag)] = current_df['avg_min_price'].mean()\n",
    "                    current_day['avg_max_price_lag_{}'.format(lag)] = current_df['avg_max_price'].mean()\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        for window in [7, 14, 30]:\n",
    "            # Use combination of historical + predicted values\n",
    "            combined_hist = list(current_df['avg_modal_price'].tail(min(window, len(current_df))))\n",
    "            \n",
    "            # Add predictions made so far if any\n",
    "            if i > 0:\n",
    "                pred_values = [p['avg_modal_price'] for p in predictions[-min(i, window-len(combined_hist)):]]\n",
    "                combined_hist.extend(pred_values)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            if combined_hist:\n",
    "                current_day['avg_modal_price_rolling_mean_{}'.format(window)] = np.mean(combined_hist[-min(window, len(combined_hist)):])\n",
    "                current_day['avg_modal_price_rolling_std_{}'.format(window)] = np.std(combined_hist[-min(window, len(combined_hist)):]) if len(combined_hist) > 1 else 0\n",
    "            else:\n",
    "                current_day['avg_modal_price_rolling_mean_{}'.format(window)] = current_df['avg_modal_price'].mean()\n",
    "                current_day['avg_modal_price_rolling_std_{}'.format(window)] = current_df['avg_modal_price'].std()\n",
    "                \n",
    "            # Repeat for min price\n",
    "            combined_hist = list(current_df['avg_min_price'].tail(min(window, len(current_df))))\n",
    "            if i > 0:\n",
    "                pred_values = [p['avg_min_price'] for p in predictions[-min(i, window-len(combined_hist)):]]\n",
    "                combined_hist.extend(pred_values)\n",
    "            \n",
    "            if combined_hist:\n",
    "                current_day['avg_min_price_rolling_mean_{}'.format(window)] = np.mean(combined_hist[-min(window, len(combined_hist)):])\n",
    "                current_day['avg_min_price_rolling_std_{}'.format(window)] = np.std(combined_hist[-min(window, len(combined_hist)):]) if len(combined_hist) > 1 else 0\n",
    "            else:\n",
    "                current_day['avg_min_price_rolling_mean_{}'.format(window)] = current_df['avg_min_price'].mean()\n",
    "                current_day['avg_min_price_rolling_std_{}'.format(window)] = current_df['avg_min_price'].std()\n",
    "            \n",
    "            # Repeat for max price\n",
    "            combined_hist = list(current_df['avg_max_price'].tail(min(window, len(current_df))))\n",
    "            if i > 0:\n",
    "                pred_values = [p['avg_max_price'] for p in predictions[-min(i, window-len(combined_hist)):]]\n",
    "                combined_hist.extend(pred_values)\n",
    "            \n",
    "            if combined_hist:\n",
    "                current_day['avg_max_price_rolling_mean_{}'.format(window)] = np.mean(combined_hist[-min(window, len(combined_hist)):])\n",
    "                current_day['avg_max_price_rolling_std_{}'.format(window)] = np.std(combined_hist[-min(window, len(combined_hist)):]) if len(combined_hist) > 1 else 0\n",
    "            else:\n",
    "                current_day['avg_max_price_rolling_mean_{}'.format(window)] = current_df['avg_max_price'].mean()\n",
    "                current_day['avg_max_price_rolling_std_{}'.format(window)] = current_df['avg_max_price'].std()\n",
    "        \n",
    "        # Make predictions using the model\n",
    "        X_day = current_day[features]\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        X_day = X_day.fillna(method='ffill')\n",
    "        if X_day.isna().any().any():\n",
    "            X_day = X_day.fillna(0)\n",
    "        \n",
    "        # Predict all three targets\n",
    "        pred = {\n",
    "            'date': current_day['full_date'].iloc[0],\n",
    "            'avg_modal_price': model['avg_modal_price'].predict(X_day)[0],\n",
    "            'avg_min_price': model['avg_min_price'].predict(X_day)[0],\n",
    "            'avg_max_price': model['avg_max_price'].predict(X_day)[0]\n",
    "        }\n",
    "        \n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Convert predictions to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # 1. First, prepare the data with time series features\n",
    "    processed_df = create_time_features(df)\n",
    "    \n",
    "    # 2. Define features for modeling \n",
    "    features = [\n",
    "        # Original features\n",
    "        'month', 'year', 'date', 'district_name_enc', 'commodity_name_enc', 'state_name_enc',\n",
    "        # Time features\n",
    "        'month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos', 'trend',\n",
    "        # Add lag and rolling statistics as needed\n",
    "    ]\n",
    "    \n",
    "    targets = ['avg_modal_price', 'avg_min_price', 'avg_max_price']\n",
    "    \n",
    "    # 3. Train ensemble model\n",
    "    models, results, feature_importances = train_xgboost_time_series()\n",
    "    \n",
    "    # 4. For a specific commodity-location combination, use SARIMA\n",
    "    sarima_model, forecast, rmse, mae = train_sarima_for_specific_combination(\n",
    "        commodity_id=1,  # Replace with actual ID\n",
    "        state_id=1,      # Replace with actual ID\n",
    "        district_id=1    # Replace with actual ID\n",
    "    )\n",
    "    \n",
    "    # 5. Forecast future prices\n",
    "    forecast_df = forecast_future_prices(models, processed_df.tail(30), features, future_days=30)\n",
    "    \n",
    "    print(\"Forecasted prices for the next 30 days:\")\n",
    "    print(forecast_df[['date', 'avg_modal_price', 'avg_min_price', 'avg_max_price']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
