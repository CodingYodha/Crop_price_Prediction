{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a67939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DeepAR Forecasting & Validation ---\n",
      "--- (Nashik/Wheat: 2002-2023 Train, 2024 Validate) ---\n",
      "------------------------------\n",
      "Processing Training (2002-2023) Dataset\n",
      "------------------------------\n",
      "Loading Training (2002-2023) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv...\n",
      "Loaded 6246 rows.\n",
      "Constructing 'full_date'...\n",
      "6246 rows after date construction.\n",
      "6246 rows after ensuring price columns numeric.\n",
      "Training (2002-2023) base data loaded. 6246 rows.\n",
      "------------------------------\n",
      "Processing Validation (2024) Dataset\n",
      "------------------------------\n",
      "Loading Validation (2024) data from E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv...\n",
      "Loaded 278 rows.\n",
      "Constructing 'full_date'...\n",
      "278 rows after date construction.\n",
      "278 rows after ensuring price columns numeric.\n",
      "Validation (2024) base data loaded. 278 rows.\n",
      "\n",
      "Selected: Maharashtra/Nashik/Wheat -> Encoded: St=6291, Di=6291, Co=6291\n",
      "\n",
      "Filtering datasets using encoded values...\n",
      "\n",
      "Preparing data for DeepAR (Target: avg_modal_price)...\n",
      "Creating TimeSeriesDataSet for DeepAR...\n",
      "TimeSeriesDataSet and Dataloaders created.\n",
      "\n",
      "Defining DeepAR model and Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepAR Network parameters: 27.7k\n",
      "\n",
      "Starting model training...\n",
      "Error during DeepAR training or evaluation: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `DeepAR`\n",
      "\n",
      "Process finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_26256\\3624202359.py\", line 282, in <module>\n",
      "    trainer.fit(\n",
      "  File \"c:\\Users\\Shiva\\.conda\\envs\\nlp_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 554, in fit\n",
      "    model = _maybe_unwrap_optimized(model)\n",
      "  File \"c:\\Users\\Shiva\\.conda\\envs\\nlp_env\\lib\\site-packages\\pytorch_lightning\\utilities\\compile.py\", line 111, in _maybe_unwrap_optimized\n",
      "    raise TypeError(\n",
      "TypeError: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `DeepAR`\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-forecasting pytorch-lightning -U # Ensure libraries are installed\n",
    "\n",
    "# --- Standard Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from IPython.display import display\n",
    "import traceback\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- PyTorch & Forecasting Imports ---\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger # Optional\n",
    "from pytorch_forecasting import TimeSeriesDataSet, DeepAR # <--- Import DeepAR\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "# Import a suitable loss function for continuous data\n",
    "from pytorch_forecasting.metrics import NormalDistributionLoss # <--- Loss for DeepAR\n",
    "\n",
    "# Suppress common warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# pl.seed_everything(42) # Optional reproducibility\n",
    "\n",
    "# --- Configuration ---\n",
    "# Data Paths\n",
    "DATA_PATH_TRAIN = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv\"\n",
    "DATA_PATH_VALIDATION = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv\"\n",
    "\n",
    "# Target Columns & Date Construction Columns\n",
    "TARGET_COLUMNS = ['avg_min_price', 'avg_max_price', 'avg_modal_price']\n",
    "PRIMARY_TARGET = 'avg_modal_price' # Focus on one target\n",
    "DATE_COLUMN = 'full_date'; YEAR_COL = 'year'; MONTH_COL = 'month'; DAY_COL = 'date'\n",
    "VALIDATION_YEAR = 2024\n",
    "\n",
    "# Filter Selections\n",
    "SELECTED_STATE_STR = \"Maharashtra\"; SELECTED_DISTRICT_STR = \"Nashik\"; SELECTED_COMMODITY_STR = \"Wheat\"\n",
    "\n",
    "# Frequency Encoding Maps (CONFIRM THESE ARE CORRECT)\n",
    "state_name_encoding_map = {\"maharashtra\": 6291}\n",
    "district_name_encoding_map = {\"nashik\": 6291}\n",
    "commodity_name_encoding_map = {\"wheat\": 6291}\n",
    "\n",
    "# --- DeepAR/Training Configuration ---\n",
    "ENCODER_LENGTH = 90      # Input sequence length (e.g., 90 days) - NEEDS TUNING\n",
    "PREDICTION_LENGTH = 1    # Predict 1 step ahead for validation\n",
    "BATCH_SIZE = 64          # Adjust based on memory\n",
    "NUM_WORKER = 0           # Use 0 for Windows generally\n",
    "TRAINER_ACCELERATOR=\"auto\" # Use \"gpu\" if available, else \"cpu\"\n",
    "TRAINER_DEVICES=\"auto\"   # Use all available devices of the chosen accelerator type\n",
    "# --- DeepAR Specific Hyperparameters (Examples - Tune these) ---\n",
    "HIDDEN_SIZE = 40\n",
    "RNN_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 0.005    # Example, needs tuning\n",
    "MAX_EPOCHS = 20          # Increase slightly for RNN? Use EarlyStopping\n",
    "\n",
    "# --- Helper Functions (remove_outliers_iqr, load_and_preprocess_base_data - NO CHANGES) ---\n",
    "def remove_outliers_iqr(df, columns_to_check):\n",
    "    df_filtered = df.copy(); initial_rows = len(df_filtered)\n",
    "    valid_columns = [col for col in columns_to_check if col in df_filtered.columns and pd.api.types.is_numeric_dtype(df_filtered[col])]\n",
    "    if not valid_columns: return df_filtered\n",
    "    subset_for_iqr = df_filtered[valid_columns]\n",
    "    Q1 = subset_for_iqr.quantile(0.25); Q3 = subset_for_iqr.quantile(0.75); IQR = Q3 - Q1\n",
    "    mask = ~((subset_for_iqr < (Q1 - 1.5 * IQR)) | (subset_for_iqr > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    df_filtered = df_filtered[mask]; rows_removed = initial_rows - len(df_filtered)\n",
    "    if rows_removed > 0: print(f\"Removed {rows_removed} rows via IQR.\")\n",
    "    return df_filtered\n",
    "\n",
    "def load_and_preprocess_base_data(path, date_col_name, year_col, month_col, day_col, target_cols, dataset_name=\"Training\"):\n",
    "    print(\"-\" * 30); print(f\"Processing {dataset_name} Dataset\"); print(\"-\" * 30)\n",
    "    try:\n",
    "        print(f\"Loading {dataset_name} data from {path}...\"); df = pd.read_csv(path); print(f\"Loaded {len(df)} rows.\")\n",
    "        # 1. Construct Date\n",
    "        date_components_cols = [year_col, month_col, day_col]\n",
    "        if not all(col in df.columns for col in date_components_cols): print(f\"Error: Date component cols missing: {[c for c in date_components_cols if c not in df.columns]}\"); return None\n",
    "        for col in date_components_cols: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=date_components_cols, inplace=True)\n",
    "        print(f\"Constructing '{date_col_name}'...\");\n",
    "        df[date_col_name] = pd.to_datetime({'year': df[year_col], 'month': df[month_col], 'day': df[day_col]}, errors='coerce')\n",
    "        initial_rows_date = len(df); df.dropna(subset=[date_col_name], inplace=True)\n",
    "        if initial_rows_date > len(df): print(f\"Dropped {initial_rows_date - len(df)} rows due to invalid date components.\")\n",
    "        print(f\"{len(df)} rows after date construction.\")\n",
    "        # 2. Initial dropna()\n",
    "        initial_rows = len(df); df.dropna(inplace=True)\n",
    "        if initial_rows > len(df): print(f\"{len(df)} rows after initial dropna(). {initial_rows - len(df)} removed.\")\n",
    "        # 3. Ensure Price columns numeric\n",
    "        all_potential_targets = ['avg_min_price', 'avg_max_price', 'avg_modal_price']\n",
    "        for col in all_potential_targets:\n",
    "            if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else: print(f\"Warning: Price column '{col}' not found.\")\n",
    "        df.dropna(subset=all_potential_targets, how='any', inplace=True)\n",
    "        print(f\"{len(df)} rows after ensuring price columns numeric.\")\n",
    "        # 4. Drop OTHER unused columns\n",
    "        cols_to_drop = ['calculationType', 'district_id', 'change','district_name_enc', 'commodity_name_enc', 'state_name_enc']\n",
    "        existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "        if existing_cols_to_drop: df.drop(columns=existing_cols_to_drop, axis=1, inplace=True)\n",
    "        # 5. Apply IQR Outlier Removal (Optional)\n",
    "        # df = remove_outliers_iqr(df, all_potential_targets)\n",
    "        # 6. Check required columns\n",
    "        required_numeric_filter_cols = ['state_name', 'district_name', 'commodity_name']\n",
    "        required_cols = [date_col_name] + all_potential_targets + required_numeric_filter_cols\n",
    "        missing_req_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_req_cols: print(f\"Error: Required columns missing: {missing_req_cols}\"); print(f\"Available: {df.columns.tolist()}\"); return None\n",
    "        for col in required_numeric_filter_cols:\n",
    "             if not pd.api.types.is_numeric_dtype(df[col]): print(f\"Error: Col '{col}' expected numeric but isn't.\"); return None\n",
    "        df.sort_values(date_col_name, inplace=True)\n",
    "        print(f\"{dataset_name} base data loaded. {len(df)} rows.\")\n",
    "        return df\n",
    "    except FileNotFoundError: print(f\"Error: {dataset_name} file not found at {path}\"); return None\n",
    "    except Exception as e: print(f\"Error loading/preprocessing {dataset_name}: {e}\"); traceback.print_exc(); return None\n",
    "\n",
    "# --- Evaluation Metrics Function (No changes) ---\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    y_true = np.array(y_true).flatten(); y_pred = np.array(y_pred).flatten()\n",
    "    valid_mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true = y_true[valid_mask]; y_pred = y_pred[valid_mask]\n",
    "    if len(y_true) == 0: print(\"Warning: No valid points for metric calculation.\"); return np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        r2 = r2_score(y_true, y_pred); mae = mean_absolute_error(y_true, y_pred); mse = mean_squared_error(y_true, y_pred)\n",
    "        return r2, mae, mse\n",
    "    except Exception as e: print(f\"Error calculating metrics: {e}\"); return np.nan, np.nan, np.nan\n",
    "\n",
    "# --- Plotting Function for Validation (No changes) ---\n",
    "def plot_validation_results(validation_actuals_df, validation_preds_df, target_column, date_col_name, title):\n",
    "    \"\"\"Plots actuals vs predictions for validation period.\"\"\"\n",
    "    fig = go.Figure(); target_label = target_column.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()\n",
    "    plot_actuals_df = validation_actuals_df.sort_values(by=date_col_name).copy()\n",
    "    plot_preds_df = validation_preds_df.sort_values(by='time_idx').copy()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=plot_actuals_df[date_col_name], y=plot_actuals_df[target_column], mode='lines+markers', name=f'Actual {target_label} ({VALIDATION_YEAR})', line=dict(color='blue'), marker=dict(size=4)))\n",
    "\n",
    "    # Merge predicted values back onto a date axis for plotting\n",
    "    pred_col_name = 'prediction' # Column name holding point forecast\n",
    "    if date_col_name not in plot_preds_df.columns:\n",
    "         time_idx_to_date = plot_actuals_df[[date_col_name, 'time_idx']].drop_duplicates().set_index('time_idx')\n",
    "         plot_preds_df = plot_preds_df.join(time_idx_to_date, on='time_idx')\n",
    "\n",
    "    if pred_col_name in plot_preds_df.columns and date_col_name in plot_preds_df.columns:\n",
    "         # Ensure prediction is 1D\n",
    "         if plot_preds_df[pred_col_name].ndim > 1:\n",
    "              pred_values = plot_preds_df[pred_col_name].iloc[:, 0] # Assume first col is point forecast\n",
    "         else:\n",
    "              pred_values = plot_preds_df[pred_col_name]\n",
    "\n",
    "         fig.add_trace(go.Scatter(x=plot_preds_df[date_col_name], y=pred_values, mode='lines', name=f'Predicted {target_label} ({VALIDATION_YEAR})', line=dict(color='red')))\n",
    "    else:\n",
    "         print(f\"Warning: Could not find '{pred_col_name}' or '{date_col_name}' in prediction results for plotting.\")\n",
    "\n",
    "    fig.update_layout(title=title, xaxis_title=f'Date ({VALIDATION_YEAR})', yaxis_title=f'Price ({target_label})', hovermode=\"x unified\", legend_title_text='Legend')\n",
    "    return fig\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "print(\"--- DeepAR Forecasting & Validation ---\") # Changed Title\n",
    "print(f\"--- (Nashik/Wheat: 2002-2023 Train, {VALIDATION_YEAR} Validate) ---\")\n",
    "\n",
    "# 1. Load Base Data\n",
    "df_train_base = load_and_preprocess_base_data(DATA_PATH_TRAIN, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, \"Training (2002-2023)\")\n",
    "df_val_base = load_and_preprocess_base_data(DATA_PATH_VALIDATION, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, f\"Validation ({VALIDATION_YEAR})\")\n",
    "\n",
    "# Init flags/variables\n",
    "train_dataloader = None; val_dataloader = None; training_dataset = None\n",
    "\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "    # 2. Get Encoded Values for Filtering\n",
    "    try:\n",
    "        selected_state_key=SELECTED_STATE_STR.strip().lower(); selected_district_key=SELECTED_DISTRICT_STR.strip().lower(); selected_commodity_key=SELECTED_COMMODITY_STR.strip().lower()\n",
    "        encoded_state = state_name_encoding_map.get(selected_state_key); encoded_district = district_name_encoding_map.get(selected_district_key); encoded_commodity = commodity_name_encoding_map.get(selected_commodity_key)\n",
    "        lookup_failed = False\n",
    "        if encoded_state is None: print(f\"Error: State '{SELECTED_STATE_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_district is None: print(f\"Error: District '{SELECTED_DISTRICT_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_commodity is None: print(f\"Error: Commodity '{SELECTED_COMMODITY_STR}' missing map.\"); lookup_failed=True\n",
    "        if lookup_failed: print(\"Check maps.\"); df_train_base=df_val_base=None\n",
    "        else: print(f\"\\nSelected: {SELECTED_STATE_STR}/{SELECTED_DISTRICT_STR}/{SELECTED_COMMODITY_STR} -> Encoded: St={encoded_state}, Di={encoded_district}, Co={encoded_commodity}\")\n",
    "    except Exception as e: print(f\"Error mapping lookup: {e}\"); df_train_base = df_val_base = None\n",
    "\n",
    "if df_train_base is not None and df_val_base is not None:\n",
    "    # 3. Filtering Data\n",
    "    print(f\"\\nFiltering datasets using encoded values...\")\n",
    "    filter_cols_num = ['state_name', 'district_name', 'commodity_name']\n",
    "    if not all(col in df_train_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Training.\"); filtered_df_train = pd.DataFrame()\n",
    "    else: filtered_df_train = df_train_base[(df_train_base['state_name'] == encoded_state) & (df_train_base['district_name'] == encoded_district) & (df_train_base['commodity_name'] == encoded_commodity)].copy(); filtered_df_train.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "    if not all(col in df_val_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Validation.\"); filtered_df_val = pd.DataFrame()\n",
    "    else: filtered_df_val = df_val_base[(df_val_base['state_name'] == encoded_state) & (df_val_base['district_name'] == encoded_district) & (df_val_base['commodity_name'] == encoded_commodity)].copy(); filtered_df_val.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "\n",
    "    if filtered_df_train.empty: print(\"\\nWarning: No training data after filtering.\")\n",
    "    if filtered_df_val.empty: print(\"\\nWarning: No validation data after filtering.\")\n",
    "\n",
    "    # 4. Prepare Data for DeepAR (Target: PRIMARY_TARGET)\n",
    "    if not filtered_df_train.empty and not filtered_df_val.empty:\n",
    "        print(f\"\\nPreparing data for DeepAR (Target: {PRIMARY_TARGET})...\")\n",
    "        try:\n",
    "            # Combine temporarily for global time_idx\n",
    "            filtered_df_train[\"dataset_type\"] = \"train\"; filtered_df_val[\"dataset_type\"] = \"val\"\n",
    "            data_comb = pd.concat([filtered_df_train, filtered_df_val], ignore_index=True).sort_values(DATE_COLUMN)\n",
    "            data_comb['time_idx'] = (data_comb[DATE_COLUMN] - data_comb[DATE_COLUMN].min()).dt.days\n",
    "            data_comb['group_id'] = f\"{SELECTED_DISTRICT_STR}_{SELECTED_COMMODITY_STR}_{PRIMARY_TARGET}\"\n",
    "            # Add time features\n",
    "            data_comb['month'] = data_comb[DATE_COLUMN].dt.month.astype(str)\n",
    "            data_comb['day_of_week'] = data_comb[DATE_COLUMN].dt.dayofweek.astype(str)\n",
    "            data_comb['day_of_month'] = data_comb[DATE_COLUMN].dt.day.astype(str)\n",
    "            data_comb['week_of_year'] = data_comb[DATE_COLUMN].dt.isocalendar().week.astype(str)\n",
    "            data_comb['year_real'] = data_comb[DATE_COLUMN].dt.year\n",
    "            data_comb[PRIMARY_TARGET] = data_comb[PRIMARY_TARGET].astype(np.float32)\n",
    "\n",
    "            # --- Feature list definitions ---\n",
    "            time_varying_known_categoricals = [\"month\", \"day_of_week\", \"day_of_month\", \"week_of_year\"]\n",
    "            time_varying_known_reals = [\"year_real\", \"time_idx\"] # time_idx is crucial here\n",
    "            # DeepAR uses past target values implicitly via RNN structure,\n",
    "            # but we still need to list the target in one of the variable groups.\n",
    "            # If ONLY target is used as unknown input, put it in time_varying_unknown_reals\n",
    "            time_varying_unknown_reals = [PRIMARY_TARGET]\n",
    "            # If using lags, add lags to time_varying_unknown_reals and target MIGHT go to known reals? Check docs.\n",
    "            static_categoricals = [\"group_id\"]\n",
    "\n",
    "            # Separate back into train and validation sets\n",
    "            train_data_tft = data_comb[data_comb[\"dataset_type\"] == \"train\"].copy()\n",
    "            first_val_idx_actual = data_comb[data_comb[\"dataset_type\"] == \"val\"]['time_idx'].min()\n",
    "            start_val_idx_with_overlap = max(0, first_val_idx_actual - ENCODER_LENGTH) # Need history for encoder\n",
    "            val_data_tft_with_overlap = data_comb[data_comb[\"time_idx\"] >= start_val_idx_with_overlap].copy()\n",
    "\n",
    "            # --- Create TimeSeriesDataSet ---\n",
    "            print(\"Creating TimeSeriesDataSet for DeepAR...\")\n",
    "            target_normalizer = GroupNormalizer(groups=[\"group_id\"], transformation=\"softplus\", center=True)\n",
    "\n",
    "            training_dataset = TimeSeriesDataSet(\n",
    "                train_data_tft,\n",
    "                time_idx=\"time_idx\", target=PRIMARY_TARGET, group_ids=[\"group_id\"],\n",
    "                max_encoder_length=ENCODER_LENGTH, max_prediction_length=PREDICTION_LENGTH,\n",
    "                static_categoricals=static_categoricals,\n",
    "                time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "                time_varying_known_reals=time_varying_known_reals,\n",
    "                time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "                target_normalizer=target_normalizer,\n",
    "                allow_missing_timesteps=True # Keep this!\n",
    "            )\n",
    "\n",
    "            validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "                training_dataset, val_data_tft_with_overlap,\n",
    "                predict=False, stop_randomization=True\n",
    "            )\n",
    "            train_dataloader = training_dataset.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKER)\n",
    "            val_dataloader = validation_dataset.to_dataloader(train=False, batch_size=BATCH_SIZE * 2, num_workers=NUM_WORKER)\n",
    "            print(\"TimeSeriesDataSet and Dataloaders created.\")\n",
    "\n",
    "        except Exception as e: print(f\"Error creating TimeSeriesDataSet: {e}\"); traceback.print_exc(); train_dataloader = val_dataloader = None\n",
    "\n",
    "    if train_dataloader is not None and val_dataloader is not None:\n",
    "        # --- 5. Define Model & Trainer ---\n",
    "        print(\"\\nDefining DeepAR model and Trainer...\")\n",
    "        try:\n",
    "            early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
    "            lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "            # logger = TensorBoardLogger(\"tb_logs\", name=\"deepar_nashik_wheat\")\n",
    "\n",
    "            trainer = pl.Trainer(\n",
    "                max_epochs=MAX_EPOCHS, accelerator=TRAINER_ACCELERATOR, devices=TRAINER_DEVICES,\n",
    "                gradient_clip_val=0.1,\n",
    "                # limit_train_batches=30, limit_val_batches=10, # For debugging\n",
    "                callbacks=[lr_monitor, early_stop_callback],\n",
    "                # logger=logger,\n",
    "                enable_progress_bar=True, enable_checkpointing=True\n",
    "            )\n",
    "\n",
    "            # --- Instantiate DeepAR Model ---\n",
    "            deepar = DeepAR.from_dataset(\n",
    "                training_dataset,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                hidden_size=HIDDEN_SIZE,\n",
    "                rnn_layers=RNN_LAYERS,\n",
    "                dropout=DROPOUT,\n",
    "                loss=NormalDistributionLoss(), # Use appropriate loss for continuous data\n",
    "                # optimizer=\"Adam\" # Default is Adam\n",
    "            )\n",
    "            print(f\"DeepAR Network parameters: {deepar.size()/1e3:.1f}k\")\n",
    "\n",
    "            # --- 6. Train Model ---\n",
    "            print(\"\\nStarting model training...\")\n",
    "            trainer.fit(\n",
    "                deepar, # Pass the deepar model object\n",
    "                train_dataloaders=train_dataloader,\n",
    "                val_dataloaders=val_dataloader,\n",
    "            )\n",
    "            print(\"Training finished.\")\n",
    "\n",
    "            # --- 7. Evaluate on Validation Set ---\n",
    "            print(f\"\\n--- Evaluating FINAL DeepAR Model Performance on {VALIDATION_YEAR} Validation Data ---\")\n",
    "            best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "            if best_model_path and os.path.exists(best_model_path):\n",
    "                 print(f\"Loading best model from: {best_model_path}\")\n",
    "                 best_model = DeepAR.load_from_checkpoint(best_model_path)\n",
    "            else:\n",
    "                 print(\"Warning: Best model checkpoint not found. Using model state from end of training.\")\n",
    "                 best_model = deepar # Use the model from the last epoch\n",
    "\n",
    "            print(\"Predicting on validation data...\")\n",
    "            # Use mode=\"prediction\" for point forecast (mean)\n",
    "            raw_predictions = best_model.predict(val_dataloader, mode=\"prediction\", return_index=True)\n",
    "            # raw_predictions is typically the tensor of predicted means here\n",
    "            # index contains time_idx and group_id\n",
    "\n",
    "            # Align actuals and predictions using the index from predict()\n",
    "            actuals_df_eval = val_data_tft_with_overlap[lambda x: x.time_idx.isin(raw_predictions.index.time_idx)][[DATE_COLUMN, 'time_idx', 'group_id', PRIMARY_TARGET]].copy()\n",
    "            predictions_df_eval = raw_predictions.index.copy()\n",
    "            # Ensure prediction tensor is correctly shaped and assigned\n",
    "            if isinstance(raw_predictions, torch.Tensor):\n",
    "                predictions_df_eval['prediction_normalized'] = raw_predictions.cpu().numpy().flatten() # Get point forecast\n",
    "            else: # Handle potential tuple output or different structure if API changes\n",
    "                 try: predictions_df_eval['prediction_normalized'] = raw_predictions.prediction.cpu().numpy().flatten()\n",
    "                 except AttributeError: print(\"Error: Could not extract predictions from model output.\"); predictions_df_eval['prediction_normalized'] = np.nan\n",
    "\n",
    "\n",
    "            eval_results = pd.merge(actuals_df_eval, predictions_df_eval, on=['time_idx', 'group_id'], how='inner')\n",
    "\n",
    "            # Inverse transform\n",
    "            target_scaler = training_dataset.target_normalizer\n",
    "            try:\n",
    "                 # Structure for inverse transform\n",
    "                 actuals_inv_input = eval_results[[PRIMARY_TARGET, 'group_id']].rename(columns={PRIMARY_TARGET: 'target'})\n",
    "                 preds_inv_input = eval_results[['prediction_normalized', 'group_id']].rename(columns={'prediction_normalized': 'target'})\n",
    "\n",
    "                 actuals_inv = target_scaler.inverse_transform(actuals_inv_input)['target'].values\n",
    "                 preds_inv = target_scaler.inverse_transform(preds_inv_input)['target'].values\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during inverse transform: {e}. Cannot calculate metrics accurately.\")\n",
    "                 actuals_inv = np.array([]) # Empty arrays to skip metric calculation\n",
    "                 preds_inv = np.array([])\n",
    "\n",
    "            # Calculate metrics\n",
    "            if len(actuals_inv) > 0 and len(preds_inv) > 0:\n",
    "                print(f\"Evaluating based on {len(actuals_inv)} matched prediction points.\")\n",
    "                r2_val, mae_val, mse_val = calculate_metrics(actuals_inv, preds_inv)\n",
    "                print(f\"FINAL Validation R-squared (R2): {r2_val:.4f}\")\n",
    "                print(f\"FINAL Validation Mean Absolute Error (MAE): {mae_val:.2f}\")\n",
    "                print(f\"FINAL Validation Mean Squared Error (MSE): {mse_val:.2f}\")\n",
    "\n",
    "                # --- 8. Plot Validation Results ---\n",
    "                print(f\"\\n--- Plotting FINAL Validation Results for {PRIMARY_TARGET} (Actual vs. Predicted {VALIDATION_YEAR}) ---\")\n",
    "                # Prepare DFs for plotting (using inverse transformed)\n",
    "                plot_actuals_df = eval_results[[DATE_COLUMN, 'time_idx']].copy(); plot_actuals_df[PRIMARY_TARGET] = actuals_inv\n",
    "                plot_preds_df = eval_results[[DATE_COLUMN, 'time_idx']].copy(); plot_preds_df['prediction'] = preds_inv\n",
    "\n",
    "                plot_title_val = f'DeepAR Validation (Nashik/Wheat): {PRIMARY_TARGET.replace(\"avg_\", \"\").replace(\"_price\", \"\").capitalize()} Price (Actual vs. Predicted {VALIDATION_YEAR})'\n",
    "                fig_val = plot_validation_results(plot_actuals_df, plot_preds_df, PRIMARY_TARGET, DATE_COLUMN, plot_title_val)\n",
    "                fig_val.show()\n",
    "            else: print(\"Skipping metrics and plotting: No matched/valid actuals/predictions found after inverse transform.\")\n",
    "\n",
    "        except Exception as e: print(f\"Error during DeepAR training or evaluation: {e}\"); traceback.print_exc()\n",
    "\n",
    "    else: print(\"\\nCannot proceed: lack of data after filtering or TimeSeriesDataSet creation failed.\")\n",
    "else: print(\"\\nFailed during data loading, preprocessing, or mapping lookup.\")\n",
    "\n",
    "print(\"\\nProcess finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
