{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40774ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# === OFFLINE TRAINING SCRIPT ===\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# (Run this in your notebook/Python environment once)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# --- Standard Imports ---\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\__init__.py:151\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    135\u001b[0m     concat,\n\u001b[0;32m    136\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     qcut,\n\u001b[0;32m    149\u001b[0m )\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\api\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     extensions,\n\u001b[0;32m      4\u001b[0m     indexers,\n\u001b[0;32m      5\u001b[0m     interchange,\n\u001b[0;32m      6\u001b[0m     types,\n\u001b[0;32m      7\u001b[0m     typing,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Expanding,\n\u001b[0;32m     21\u001b[0m     ExpandingGroupby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     Window,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[0;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameGroupBy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndexResamplerGroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\json\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     read_json,\n\u001b[0;32m      3\u001b[0m     to_json,\n\u001b[0;32m      4\u001b[0m     ujson_dumps,\n\u001b[0;32m      5\u001b[0m     ujson_loads,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_dumps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_loads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_table_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\json\\_json.py:71\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_normalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     build_table_schema,\n\u001b[0;32m     69\u001b[0m     parse_table_schema,\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m         Hashable,\n\u001b[0;32m     76\u001b[0m         Mapping,\n\u001b[0;32m     77\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     TextFileReader,\n\u001b[0;32m      3\u001b[0m     TextParser,\n\u001b[0;32m      4\u001b[0m     read_csv,\n\u001b[0;32m      5\u001b[0m     read_fwf,\n\u001b[0;32m      6\u001b[0m     read_table,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextParser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     AbstractMethodError,\n\u001b[0;32m     35\u001b[0m     ParserWarning,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
      "File \u001b[1;32mparsers.pyx:1418\u001b[0m, in \u001b[0;36minit pandas._libs.parsers\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# === OFFLINE TRAINING SCRIPT ===\n",
    "# (Run this in your notebook/Python environment once)\n",
    "\n",
    "# --- Standard Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import traceback\n",
    "import warnings\n",
    "import os\n",
    "import joblib # <--- For saving scaler\n",
    "\n",
    "# --- Keras / TensorFlow Imports ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam # Using TF Keras path\n",
    "\n",
    "\n",
    "# Suppress common warnings & TF logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# tf.random.set_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH_TRAIN = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_2002_2023.csv\"\n",
    "# DATA_PATH_VALIDATION = r\"E:\\elevatetrsest\\crop price predictor\\Crop_price_Prediction\\data\\edited_nashik_test_2024.csv\" # Not needed for training only\n",
    "TARGET_COLUMNS = ['avg_min_price', 'avg_max_price', 'avg_modal_price'] # Train for all targets\n",
    "DATE_COLUMN = 'full_date'; YEAR_COL = 'year'; MONTH_COL = 'month'; DAY_COL = 'date'\n",
    "# Filter Selections\n",
    "SELECTED_STATE_STR = \"Maharashtra\"; SELECTED_DISTRICT_STR = \"Nashik\"; SELECTED_COMMODITY_STR = \"Wheat\"\n",
    "# Frequency Encoding Maps\n",
    "state_name_encoding_map = {\"maharashtra\": 6291}\n",
    "district_name_encoding_map = {\"nashik\": 6291}\n",
    "commodity_name_encoding_map = {\"wheat\": 6291}\n",
    "# LSTM Config\n",
    "SEQUENCE_LENGTH = 60\n",
    "LSTM_UNITS = 50 # Use fixed HPs found from tuning or reasonable defaults\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 0.001 # Use fixed HPs found from tuning or reasonable defaults\n",
    "EPOCHS = 100 # Train longer offline, use EarlyStopping\n",
    "BATCH_SIZE = 32\n",
    "# Save Directory\n",
    "MODEL_SAVE_DIR = \"saved_lstm_models\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True) # Create directory if it doesn't exist\n",
    "\n",
    "# --- Helper Functions (load_and_preprocess_base_data, create_sequences) ---\n",
    "# (Copy these functions from the previous script - no changes needed in them)\n",
    "def remove_outliers_iqr(df, columns_to_check):\n",
    "    # ... (implementation from previous script) ...\n",
    "    df_filtered = df.copy(); initial_rows = len(df_filtered)\n",
    "    valid_columns = [col for col in columns_to_check if col in df_filtered.columns and pd.api.types.is_numeric_dtype(df_filtered[col])]\n",
    "    if not valid_columns: return df_filtered\n",
    "    subset_for_iqr = df_filtered[valid_columns]\n",
    "    Q1 = subset_for_iqr.quantile(0.25); Q3 = subset_for_iqr.quantile(0.75); IQR = Q3 - Q1\n",
    "    mask = ~((subset_for_iqr < (Q1 - 1.5 * IQR)) | (subset_for_iqr > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    df_filtered = df_filtered[mask]; rows_removed = initial_rows - len(df_filtered)\n",
    "    if rows_removed > 0: print(f\"Removed {rows_removed} rows via IQR.\")\n",
    "    return df_filtered\n",
    "\n",
    "def load_and_preprocess_base_data(path, date_col_name, year_col, month_col, day_col, all_potential_targets, dataset_name=\"Training\"):\n",
    "    # ... (implementation from previous script) ...\n",
    "    print(\"-\" * 30); print(f\"Processing {dataset_name} Dataset\"); print(\"-\" * 30)\n",
    "    try:\n",
    "        print(f\"Loading {dataset_name} data from {path}...\"); df = pd.read_csv(path); print(f\"Loaded {len(df)} rows.\")\n",
    "        date_components_cols = [year_col, month_col, day_col]\n",
    "        if not all(col in df.columns for col in date_components_cols): print(f\"Error: Date component cols missing: {[c for c in date_components_cols if c not in df.columns]}\"); return None\n",
    "        for col in date_components_cols: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=date_components_cols, inplace=True)\n",
    "        print(f\"Constructing '{date_col_name}'...\");\n",
    "        df[date_col_name] = pd.to_datetime({'year': df[year_col], 'month': df[month_col], 'day': df[day_col]}, errors='coerce')\n",
    "        initial_rows_date = len(df); df.dropna(subset=[date_col_name], inplace=True)\n",
    "        if initial_rows_date > len(df): print(f\"Dropped {initial_rows_date - len(df)} rows due to invalid date components.\")\n",
    "        print(f\"{len(df)} rows after date construction.\")\n",
    "        required_numeric_filter_cols = ['state_name', 'district_name', 'commodity_name']\n",
    "        keep_cols = [date_col_name] + all_potential_targets + required_numeric_filter_cols\n",
    "        missing_req_cols = [col for col in keep_cols if col not in df.columns]\n",
    "        if missing_req_cols: print(f\"Error: Required columns missing: {missing_req_cols}\"); print(f\"Available: {df.columns.tolist()}\"); return None\n",
    "        df = df[keep_cols]\n",
    "        for col in all_potential_targets: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=all_potential_targets, how='any', inplace=True)\n",
    "        print(f\"{len(df)} rows after ensuring price columns numeric.\")\n",
    "        for col in required_numeric_filter_cols:\n",
    "             if not pd.api.types.is_numeric_dtype(df[col]): print(f\"Error: Col '{col}' expected numeric but isn't.\"); return None\n",
    "        # df = remove_outliers_iqr(df, all_potential_targets) # Optional\n",
    "        df.sort_values(date_col_name, inplace=True)\n",
    "        print(f\"{dataset_name} base data loaded. {len(df)} rows.\")\n",
    "        return df\n",
    "    except FileNotFoundError: print(f\"Error: {dataset_name} file not found at {path}\"); return None\n",
    "    except Exception as e: print(f\"Error loading/preprocessing {dataset_name}: {e}\"); traceback.print_exc(); return None\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    # ... (implementation from previous script) ...\n",
    "    X, y = [], []\n",
    "    if len(data) <= sequence_length: return np.array(X), np.array(y)\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:(i + sequence_length)])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# --- Main Training & Saving Logic ---\n",
    "print(\"--- LSTM Offline Training & Saving ---\")\n",
    "\n",
    "# 1. Load Training Data Only\n",
    "df_train_base = load_and_preprocess_base_data(DATA_PATH_TRAIN, DATE_COLUMN, YEAR_COL, MONTH_COL, DAY_COL, TARGET_COLUMNS, \"Training (2002-2023)\")\n",
    "\n",
    "if df_train_base is not None:\n",
    "    # 2. Get Encoded Values\n",
    "    try:\n",
    "        # ... (encoding map lookup - same as before) ...\n",
    "        selected_state_key=SELECTED_STATE_STR.strip().lower(); selected_district_key=SELECTED_DISTRICT_STR.strip().lower(); selected_commodity_key=SELECTED_COMMODITY_STR.strip().lower()\n",
    "        encoded_state = state_name_encoding_map.get(selected_state_key); encoded_district = district_name_encoding_map.get(selected_district_key); encoded_commodity = commodity_name_encoding_map.get(selected_commodity_key)\n",
    "        # ... (error checking for lookup - same as before) ...\n",
    "        lookup_failed = False\n",
    "        if encoded_state is None: print(f\"Error: State '{SELECTED_STATE_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_district is None: print(f\"Error: District '{SELECTED_DISTRICT_STR}' missing map.\"); lookup_failed=True\n",
    "        if encoded_commodity is None: print(f\"Error: Commodity '{SELECTED_COMMODITY_STR}' missing map.\"); lookup_failed=True\n",
    "        if lookup_failed: print(\"Check maps.\"); df_train_base=None\n",
    "        else: print(f\"\\nSelected: {SELECTED_STATE_STR}/{SELECTED_DISTRICT_STR}/{SELECTED_COMMODITY_STR} -> Encoded: St={encoded_state}, Di={encoded_district}, Co={encoded_commodity}\")\n",
    "\n",
    "    except Exception as e: print(f\"Error mapping lookup: {e}\"); df_train_base = None\n",
    "\n",
    "if df_train_base is not None:\n",
    "    # 3. Filtering Data\n",
    "    print(f\"\\nFiltering dataset using encoded values...\")\n",
    "    filter_cols_num = ['state_name', 'district_name', 'commodity_name']\n",
    "    if not all(col in df_train_base.columns for col in filter_cols_num): print(\"Error: Encoded filter cols missing Training.\"); filtered_df_train = pd.DataFrame()\n",
    "    else: filtered_df_train = df_train_base[(df_train_base['state_name'] == encoded_state) & (df_train_base['district_name'] == encoded_district) & (df_train_base['commodity_name'] == encoded_commodity)].copy(); filtered_df_train.sort_values(by=DATE_COLUMN, inplace=True)\n",
    "\n",
    "    if filtered_df_train.empty:\n",
    "        print(\"\\nError: No training data found after filtering. Cannot train models.\")\n",
    "    else:\n",
    "        print(f\"Filtered training data shape: {filtered_df_train.shape}\")\n",
    "\n",
    "        # --- Loop through targets to train and save ---\n",
    "        for target in TARGET_COLUMNS:\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Processing and training for target: {target}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            # 4. Prepare Data for LSTM\n",
    "            train_series = filtered_df_train[target].values.reshape(-1, 1)\n",
    "            if len(train_series) <= SEQUENCE_LENGTH:\n",
    "                print(f\"Skipping {target}: Not enough data points ({len(train_series)}) after filtering to create sequences.\")\n",
    "                continue\n",
    "\n",
    "            # Scale data\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaled_train_data = scaler.fit_transform(train_series)\n",
    "\n",
    "            # Create sequences\n",
    "            X_train, y_train = create_sequences(scaled_train_data, SEQUENCE_LENGTH)\n",
    "\n",
    "            if X_train.shape[0] == 0:\n",
    "                print(f\"Skipping {target}: Not enough data to create sequences ({len(scaled_train_data)} points).\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Training sequences shape for {target}: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "            # --- 5. Build & Train LSTM Model ---\n",
    "            print(f\"\\nBuilding LSTM model for {target}...\")\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(LSTM_UNITS, activation='relu', input_shape=(SEQUENCE_LENGTH, 1)))\n",
    "            model.add(Dropout(DROPOUT_RATE))\n",
    "            model.add(Dense(1))\n",
    "            optimizer = Adam(learning_rate=LEARNING_RATE) # Use configured LR\n",
    "            model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "            # model.summary() # Optional summary\n",
    "\n",
    "            print(f\"\\nStarting LSTM model training for {target}...\")\n",
    "            # Use a portion of training data for validation during offline training if desired\n",
    "            # Or simply train until convergence / max epochs\n",
    "            # early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True) # Stop based on training loss plateau\n",
    "            # For simplicity, train for fixed epochs here, but EarlyStopping on val_loss is better if you create a split\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                # validation_split=0.1, # Use 10% of training sequences for validation during fit\n",
    "                # callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            print(f\"Training finished for {target}.\")\n",
    "\n",
    "            # --- 6. Save Model, Scaler, Last Sequence ---\n",
    "            print(f\"\\nSaving artifacts for {target}...\")\n",
    "            try:\n",
    "                # Save Model\n",
    "                model_path = os.path.join(MODEL_SAVE_DIR, f\"lstm_model_{target}.h5\")\n",
    "                model.save(model_path)\n",
    "                print(f\"   Model saved to: {model_path}\")\n",
    "\n",
    "                # Save Scaler\n",
    "                scaler_path = os.path.join(MODEL_SAVE_DIR, f\"scaler_{target}.joblib\")\n",
    "                joblib.dump(scaler, scaler_path)\n",
    "                print(f\"   Scaler saved to: {scaler_path}\")\n",
    "\n",
    "                # Save Last Sequence (Needed to start predictions)\n",
    "                last_sequence = scaled_train_data[-SEQUENCE_LENGTH:]\n",
    "                sequence_path = os.path.join(MODEL_SAVE_DIR, f\"last_sequence_{target}.npy\")\n",
    "                np.save(sequence_path, last_sequence)\n",
    "                print(f\"   Last sequence saved to: {sequence_path} (Shape: {last_sequence.shape})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving artifacts for {target}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Offline training and artifact saving complete.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nFailed during data loading, preprocessing, or mapping lookup. Cannot train.\")\n",
    "\n",
    "print(\"\\nProcess finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
